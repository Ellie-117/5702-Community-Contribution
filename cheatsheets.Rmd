# Cheat Sheets

## Basic Function

-   as_bench_result(x, ) converts object to benchmark result for visualization

-   mlr3 Dictionaries The dictionaries stores all the classes with functions that we can use in this mlr3 library.

    -mlr_tasks

    -mlr_task_generators

    -mlr_learners

    -mlr_measures

    -mlr_resampling

Example usages: The function keys() returns all learners keys prebuilt in mlr3 package. If we install mlr3learners package, we will get an extension version of it. Installing the according libraries and extend the dictionaries.

```{r,echo = TRUE}
library(mlr3)
mlr_learners$keys(pattern = NULL)
```

For a brief introduction, we will explain the keywords in these six to get a better understanding:

-classif means it is used to solve classification related problems, regr means it is used to solve regression related problems.

-featureless means that the learner will ignore all the features during train and only consider the response.

-classif.rpart is a LearnerClassif for a classification tree implemented and regr.rpart is a LearnerRegr for a regression tree implemented. These two functions will take in features during training.

-debug learner used for debugging purposes.

The function get() retrieves object by key. It will show all the information about the key.

```{r,echo = TRUE}
mlr_learners$get("classif.rpart")
```

The function makes dictionary to data.table form.

```{r,echo = TRUE}
head(as.data.table(mlr_tasks))
```

-   Tasks

Target determines the machine learning Task. We can create a classification task:

```{r,echo = TRUE}
task1 = as_task_classif(x = iris, target = "Species")
task1
```

We can create a regression task:

```{r,echo = TRUE}
age <- c(33, 55, 25)
salary <- c(20000, 50000, 15000)
df <- data.frame(age, salary)
task2 = as_task_regr(x= df, target = "salary")
task2
```

We can also use the example tasks in mlr_tasks by calling tsk(task_name):

```{r,echo = TRUE}
task3 = tsk("zoo")
task3
```

We can perform some functions on the task: task\$positive = "<argument>" sets positive class for binary classification

```{r,echo = TRUE}
#return number of rows
task1$nrow
```

```{r,echo = TRUE}
#return number of columns
task1$ncol
```

```{r,echo = TRUE}
#subset the task by selecting features
task1$select("Sepal.Length")
```

task\$cbind(data) adds columns

task\$rbind(data) adds rows

task\$feature_names return feature names in the task

-   Learner

To use a learner, we can call the method using:

learner = mlr_learners\$get(method) or

learner = lrn(method)

Here is an example:

```{r,echo = TRUE}
learner = lrn("regr.rpart")
learner
```

-   Train

We train our task using the learner we chose:

learner\$train(task, row_ids)

learner\$model: the model is stored and viewed

Split on test/train:

train_set = sample(task\$nrow, (percentage) \* my_task\$nrow)

test_set = setdiff(seq_len(task\$nrow), train_set)

-   Predict

These two methods will predict on the select data:
prediction = learner\$predict(task, row_ids)

prediction = learner\$predict_newdata(data)

-   Model Evaluation

Here are the model evaluation metrics in the mlr_measures library:

```{r,echo = TRUE}
mlr_measures$keys(pattern = NULL)
```

prediction\$score(measures): returns the model evaluation metrics of the selected learner

## Pipeline

## Hyperparameter Tuning

```{r,echo = TRUE}
library(mlr3learners)
library(mlr3tuning)
```

The table shows the terminator methods:

```{r}
as.data.table(mlr_terminators)
```

The table shows tuner search strategy we can choose from:

```{r,echo = TRUE}
as.data.table(mlr_tuners) 
```

The parameter set is combined in mutivariate search space SS:
ss = ps(\<id\> = p_int(lower, upper), \<id\>= p_dbl(lower, upper), \<id\> = p_dct(levels), \<id\> = p_lgl())

The \<id\> represents identifier, and lower, upper, levels are the bounds.

Or, we can use to_tune() to set SS for each parameter.

To tune by hand, we need to fill define all the arguments in the equation:
instance = TuningInstanceSingleCrit\$new(task,learner, resampling, measure,terminator, ss)
tuner = tnr(\<tuner\>)
We need to use TunningInstanceMultiCrit for multi-criteria tuning.

Then we access the results:

tuner\$optimize(instance)
as.data.table(instance\$archive)
learner\$param_set\$values = instance\$result_learner_param_vals

The auto tuner:

auto_tuner(

method = tnr(\<tuner search strategy\>),

learner = lrn(\<learner\>, cp = to_tune(lower bound, upperbound, logscale = \<TRUE/FALSE\>)),

resampling = rsmp(\<method\>),

measure = msr(\<measure\>),

term_evals = \<#\>,

batch_size = \<#\>

)

## Feature Selection

```{r,echo = TRUE}
library(mlr3fselect)

```

Here is the auto feature selectorï¼š

The table shows fselectors method we can choose from:

```{r, echo =TRUE}
as.data.table(mlr_fselectors) 
```

First, we make feature selection by hand, and the process is similar to hyper parameter tuning, we need to define all the arguments and then we can get the result:
instance = FSelectInstanceSingleCrit\$new(task, learner, resampling, measure, terminator)
fselector = fs(\<fs method\>, batch_size = \<number\>)
fselector\$optimize(instance)
instance\$result

We can reselect the features we want using the code:
task\$select(instance\$result_feature_set)

Next, we will introduce the auto feature selector that eases the process:

autot=auto_fselector(

method = \<fselector\>,

learner = \<your learner\>,

resampling = rsmp(\<method\>),

measure = msr(\<measure\>),

term_evals = \<#\>,

batch_size = \<#\>)
autot\$train(task, row_ids)
autot\$predict(task, row_ids)

We can check the feature selection subset by calling the learner again:

autot\$learner
