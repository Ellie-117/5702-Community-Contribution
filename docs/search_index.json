[["index.html", "Community Contribution – mlr3 tutorial Chapter 1 Introduction 1.1 Background 1.2 Target Users 1.3 Development", " Community Contribution – mlr3 tutorial Lingjun Zhang, Michelle Sun 2022-11-15 Chapter 1 Introduction 1.1 Background The mlr3 (Lang et al. 2019) package and ecosystem provide a generic, object-oriented, and extensible framework for classification, regression, survival analysis, and other machine learning tasks for the R language (R Core Team 2019). This unified interface provides functionality to extend and combine existing machine learning algorithms (learners), intelligently select and tune the most appropriate technique for a specific machine learning task, and perform large-scale comparisons that enable meta-learning. Examples of this advanced functionality include hyperparameter tuning and feature selection. Parallelization of many operations is natively supported. 1.2 Target Users We assume that users of mlr3 have the equivalent knowledge of an introductory machine learning course and some experience in R. A background in computer science or statistics will provide a strong basis for understanding the advanced functionality described in the later chapters of this book. “An Introduction to Statistical Learning” provides a comprehensive introduction for those getting started in machine learning. mlr3 is suitable for complex projects that utilize the high degree of control as well as the highly abstracted “syntactic sugar” to mock-up specific tasks. mlr3 provides a domain-specific language for machine learning in R. We target both practitioners who want to quickly apply machine learning algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment. 1.3 Development mlr (Bischl et al. 2016) was first released to CRAN in 2013, with the core design and architecture dating back much further. Over time, the addition of many features has led to a considerably more complex design that made it harder to build, maintain, and extend than we had hoped for. With hindsight, we saw that some design and architecture choices in mlr made it difficult to support new features, in particular with respect to pipelines. Furthermore, the R ecosystem as well as helpful packages such as data.table have undergone major changes in the meantime. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. It would have been nearly impossible to integrate all of these changes into the original design of mlr. Instead, we decided to start working on a reimplementation in 2018, which resulted in the first release of mlr3 on CRAN in July 2019. The new design and the integration of further and newly-developed R packages (especially R6, future, and data.table) makes mlr3 much easier to use, maintain, and in many regards more efficient compared to its predecessor mlr. "],["package-prepare.html", "Chapter 2 Package Prepare 2.1 Installation 2.2 Package Ecosystem 2.3 Packages Installation", " Chapter 2 Package Prepare 2.1 Installation We recommend installing the full universe at once: install.packages(&quot;mlr3verse&quot;) You can also just install the base package: install.packages(&quot;mlr3&quot;) 2.2 Package Ecosystem mlr3 makes use of the following packages not developed by core members of the mlr3 team: R6: Reference class objects. data.table: Extension of R’s data.frame. digest: Hash digests. uuid: Unique string identifiers. lgr: Logging facility. mlbench: A collection of machine learning data sets. evaluate: For capturing output, warnings, and exceptions. future / future.apply: For parallelization. These are core packages within the R ecosystem. The mlr3 package itself provides the base functionality that the rest of ecosystem (mlr3verse) rely on and some fundamental building blocks for machine learning. 2.3 Packages Installation All packages in the mlr3 ecosystem can be installed from GitHub and R-universe and the majority (but not all) can be installed from CRAN. We recommend adding the mlr-org R-universe1 to your R options so that you can install all packages with install.packages() without having to worry whether it’s being downloaded from CRAN or R-universe. To do this run the following: usethis::edit_r_profile() And in the file that opens add or change the repos argument in options so it looks something like this (you might need to add the full code block below or just edit the existing options function). options(repos = c( mlrorg = &quot;https://mlr-org.r-universe.dev&quot;, CRAN = &quot;https://cloud.r-project.org/&quot; )) Save the file then restart your R session and you’re ready to go! install.packages(&quot;mlr3verse&quot;) If you want latest development versions of any of our packages you can just run remotes::install_github(&quot;mlr-org/{pkg}&quot;) "],["cheat-sheets.html", "Chapter 3 Cheat Sheets 3.1 Basic Function 3.2 Pipeline 3.3 Hyperparameter Tuning 3.4 Feature Selection", " Chapter 3 Cheat Sheets 3.1 Basic Function as_bench_result(x, ) converts object to benchmark result for visualization mlr3 Dictionaries The dictionaries stores all the classes with functions that we can use in this mlr3 library. -mlr_tasks -mlr_task_generators -mlr_learners -mlr_measures -mlr_resampling Example usages: The function keys() returns all learners keys prebuilt in mlr3 package. If we install mlr3learners package, we will get an extension version of it. Installing the according libraries and extend the dictionaries. library(mlr3) mlr_learners$keys(pattern = NULL) ## [1] &quot;classif.cv_glmnet&quot; &quot;classif.debug&quot; &quot;classif.featureless&quot; ## [4] &quot;classif.glmnet&quot; &quot;classif.kknn&quot; &quot;classif.lda&quot; ## [7] &quot;classif.log_reg&quot; &quot;classif.multinom&quot; &quot;classif.naive_bayes&quot; ## [10] &quot;classif.nnet&quot; &quot;classif.qda&quot; &quot;classif.ranger&quot; ## [13] &quot;classif.rpart&quot; &quot;classif.svm&quot; &quot;classif.xgboost&quot; ## [16] &quot;regr.cv_glmnet&quot; &quot;regr.debug&quot; &quot;regr.featureless&quot; ## [19] &quot;regr.glmnet&quot; &quot;regr.kknn&quot; &quot;regr.km&quot; ## [22] &quot;regr.lm&quot; &quot;regr.nnet&quot; &quot;regr.ranger&quot; ## [25] &quot;regr.rpart&quot; &quot;regr.svm&quot; &quot;regr.xgboost&quot; For a brief introduction, we will explain the keywords in these six to get a better understanding: -classif means it is used to solve classification related problems, regr means it is used to solve regression related problems. -featureless means that the learner will ignore all the features during train and only consider the response. -classif.rpart is a LearnerClassif for a classification tree implemented and regr.rpart is a LearnerRegr for a regression tree implemented. These two functions will take in features during training. -debug learner used for debugging purposes. The function get() retrieves object by key. It will show all the information about the key. mlr_learners$get(&quot;classif.rpart&quot;) ## &lt;LearnerClassifRpart:classif.rpart&gt;: Classification Tree ## * Model: - ## * Parameters: xval=0 ## * Packages: mlr3, rpart ## * Predict Types: [response], prob ## * Feature Types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, multiclass, selected_features, ## twoclass, weights The function makes dictionary to data.table form. head(as.data.table(mlr_tasks)) ## key label task_type nrow ncol properties lgl ## 1: boston_housing Boston Housing Prices regr 506 19 0 ## 2: breast_cancer Wisconsin Breast Cancer classif 683 10 twoclass 0 ## 3: german_credit German Credit classif 1000 21 twoclass 0 ## 4: iris Iris Flowers classif 150 5 multiclass 0 ## 5: mtcars Motor Trends regr 32 11 0 ## 6: penguins Palmer Penguins classif 344 8 multiclass 0 ## int dbl chr fct ord pxc ## 1: 3 13 0 2 0 0 ## 2: 0 0 0 0 9 0 ## 3: 3 0 0 14 3 0 ## 4: 0 4 0 0 0 0 ## 5: 0 10 0 0 0 0 ## 6: 3 2 0 2 0 0 Tasks Target determines the machine learning Task. We can create a classification task: task1 = as_task_classif(x = iris, target = &quot;Species&quot;) task1 ## &lt;TaskClassif:iris&gt; (150 x 5) ## * Target: Species ## * Properties: multiclass ## * Features (4): ## - dbl (4): Petal.Length, Petal.Width, Sepal.Length, Sepal.Width We can create a regression task: age &lt;- c(33, 55, 25) salary &lt;- c(20000, 50000, 15000) df &lt;- data.frame(age, salary) task2 = as_task_regr(x= df, target = &quot;salary&quot;) task2 ## &lt;TaskRegr:df&gt; (3 x 2) ## * Target: salary ## * Properties: - ## * Features (1): ## - dbl (1): age We can also use the example tasks in mlr_tasks by calling tsk(task_name): task3 = tsk(&quot;zoo&quot;) task3 ## &lt;TaskClassif:zoo&gt; (101 x 17): Zoo Animals ## * Target: type ## * Properties: multiclass ## * Features (16): ## - lgl (15): airborne, aquatic, backbone, breathes, catsize, domestic, ## eggs, feathers, fins, hair, milk, predator, tail, toothed, venomous ## - int (1): legs We can perform some functions on the task: task$positive = “” sets positive class for binary classification #return number of rows task1$nrow ## [1] 150 #return number of columns task1$ncol ## [1] 5 #subset the task by selecting features task1$select(&quot;Sepal.Length&quot;) task$cbind(data) adds columns task$rbind(data) adds rows task$feature_names return feature names in the task Learner To use a learner, we can call the method using: learner = mlr_learners$get(method) or learner = lrn(method) Here is an example: learner = lrn(&quot;regr.rpart&quot;) learner ## &lt;LearnerRegrRpart:regr.rpart&gt;: Regression Tree ## * Model: - ## * Parameters: xval=0 ## * Packages: mlr3, rpart ## * Predict Types: [response] ## * Feature Types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, selected_features, weights Train We train our task using the learner we chose: learner$train(task, row_ids) learner$model: the model is stored and viewed Split on test/train: train_set = sample(task$nrow, (percentage) * my_task$nrow) test_set = setdiff(seq_len(task$nrow), train_set) Predict These two methods will predict on the select data: prediction = learner$predict(task, row_ids) prediction = learner$predict_newdata(data) Model Evaluation Here are the model evaluation metrics in the mlr_measures library: mlr_measures$keys(pattern = NULL) ## [1] &quot;aic&quot; &quot;bic&quot; &quot;classif.acc&quot; ## [4] &quot;classif.auc&quot; &quot;classif.bacc&quot; &quot;classif.bbrier&quot; ## [7] &quot;classif.ce&quot; &quot;classif.costs&quot; &quot;classif.dor&quot; ## [10] &quot;classif.fbeta&quot; &quot;classif.fdr&quot; &quot;classif.fn&quot; ## [13] &quot;classif.fnr&quot; &quot;classif.fomr&quot; &quot;classif.fp&quot; ## [16] &quot;classif.fpr&quot; &quot;classif.logloss&quot; &quot;classif.mauc_au1p&quot; ## [19] &quot;classif.mauc_au1u&quot; &quot;classif.mauc_aunp&quot; &quot;classif.mauc_aunu&quot; ## [22] &quot;classif.mbrier&quot; &quot;classif.mcc&quot; &quot;classif.npv&quot; ## [25] &quot;classif.ppv&quot; &quot;classif.prauc&quot; &quot;classif.precision&quot; ## [28] &quot;classif.recall&quot; &quot;classif.sensitivity&quot; &quot;classif.specificity&quot; ## [31] &quot;classif.tn&quot; &quot;classif.tnr&quot; &quot;classif.tp&quot; ## [34] &quot;classif.tpr&quot; &quot;debug&quot; &quot;oob_error&quot; ## [37] &quot;regr.bias&quot; &quot;regr.ktau&quot; &quot;regr.mae&quot; ## [40] &quot;regr.mape&quot; &quot;regr.maxae&quot; &quot;regr.medae&quot; ## [43] &quot;regr.medse&quot; &quot;regr.mse&quot; &quot;regr.msle&quot; ## [46] &quot;regr.pbias&quot; &quot;regr.rae&quot; &quot;regr.rmse&quot; ## [49] &quot;regr.rmsle&quot; &quot;regr.rrse&quot; &quot;regr.rse&quot; ## [52] &quot;regr.rsq&quot; &quot;regr.sae&quot; &quot;regr.smape&quot; ## [55] &quot;regr.srho&quot; &quot;regr.sse&quot; &quot;selected_features&quot; ## [58] &quot;sim.jaccard&quot; &quot;sim.phi&quot; &quot;time_both&quot; ## [61] &quot;time_predict&quot; &quot;time_train&quot; prediction$score(measures): returns the model evaluation metrics of the selected learner 3.2 Pipeline Machine learning workflows can be written as directed “Graphs”/“Pipelines” that represent data flows between preprocessing, model fitting, and ensemble learning units in an expressive and intuitive language. We will most often use the term “Graph” in this manual but it can interchangeably be used with “pipeline” or “workflow”. Single computational steps can be represented as so-called PipeOps, which can then be connected with directed edges in a Graph. The scope of mlr3pipelines is still growing. Currently supported features are: Data manipulation and preprocessing operations, e.g. PCA, feature filtering, imputation Task subsampling for speed and outcome class imbalance handling mlr3 Learner operations for prediction and stacking Ensemble methods and aggregation of predictions Additionally, we implement several meta operators that can be used to construct powerful pipelines: Simultaneous path branching (data going both ways) Alternative path branching (data going one specific way, controlled by hyperparameters) Using methods from mlr3tuning, it is even possible to simultaneously optimize parameters of multiple processing units. 3.2.1 The Building Blocks: PipeOps The building blocks of mlr3pipelines are PipeOp-objects (PO). They can be constructed directly using PipeOp&lt;NAME&gt;$new(), but the recommended way is to retrieve them from the mlr_pipeops dictionary: library(&quot;mlr3pipelines&quot;) as.data.table(mlr_pipeops) ## key ## 1: boxcox ## 2: branch ## 3: chunk ## 4: classbalancing ## 5: classifavg ## 6: classweights ## 7: colapply ## 8: collapsefactors ## 9: colroles ## 10: copy ## 11: datefeatures ## 12: encode ## 13: encodeimpact ## 14: encodelmer ## 15: featureunion ## 16: filter ## 17: fixfactors ## 18: histbin ## 19: ica ## 20: imputeconstant ## 21: imputehist ## 22: imputelearner ## 23: imputemean ## 24: imputemedian ## 25: imputemode ## 26: imputeoor ## 27: imputesample ## 28: kernelpca ## 29: learner ## 30: learner_cv ## 31: missind ## 32: modelmatrix ## 33: multiplicityexply ## 34: multiplicityimply ## 35: mutate ## 36: nmf ## 37: nop ## 38: ovrsplit ## 39: ovrunite ## 40: pca ## 41: proxy ## 42: quantilebin ## 43: randomprojection ## 44: randomresponse ## 45: regravg ## 46: removeconstants ## 47: renamecolumns ## 48: replicate ## 49: scale ## 50: scalemaxabs ## 51: scalerange ## 52: select ## 53: smote ## 54: spatialsign ## 55: subsample ## 56: targetinvert ## 57: targetmutate ## 58: targettrafoscalerange ## 59: textvectorizer ## 60: threshold ## 61: tunethreshold ## 62: unbranch ## 63: vtreat ## 64: yeojohnson ## key ## label ## 1: Box-Cox Transformation of Numeric Features ## 2: Path Branching ## 3: Chunk Input into Multiple Outputs ## 4: Class Balancing ## 5: Majority Vote Prediction ## 6: Class Weights for Sample Weighting ## 7: Apply a Function to each Column of a Task ## 8: Collapse Factors ## 9: Change Column Roles of a Task ## 10: Copy Input Multiple Times ## 11: Preprocess Date Features ## 12: Factor Encoding ## 13: Conditional Target Value Impact Encoding ## 14: Impact Encoding with Random Intercept Models ## 15: Aggregate Features from Multiple Inputs ## 16: Feature Filtering ## 17: Fix Factor Levels ## 18: Split Numeric Features into Equally Spaced Bins ## 19: Independent Component Analysis ## 20: Impute Features by a Constant ## 21: Impute Numerical Features by Histogram ## 22: Impute Features by Fitting a Learner ## 23: Impute Numerical Features by their Mean ## 24: Impute Numerical Features by their Median ## 25: Impute Features by their Mode ## 26: Out of Range Imputation ## 27: Impute Features by Sampling ## 28: Kernelized Principle Component Analysis ## 29: Wrap a Learner into a PipeOp ## 30: Wrap a Learner into a PipeOp with Cross-validated Predictions as Features ## 31: Add Missing Indicator Columns ## 32: Transform Columns by Constructing a Model Matrix ## 33: Explicate a Multiplicity ## 34: Implicate a Multiplicity ## 35: Add Features According to Expressions ## 36: Non-negative Matrix Factorization ## 37: Simply Push Input Forward ## 38: Split a Classification Task into Binary Classification Tasks ## 39: Unite Binary Classification Tasks ## 40: Principle Component Analysis ## 41: Wrap another PipeOp or Graph as a Hyperparameter ## 42: Split Numeric Features into Quantile Bins ## 43: Project Numeric Features onto a Randomly Sampled Subspace ## 44: Generate a Randomized Response Prediction ## 45: Weighted Prediction Averaging ## 46: Remove Constant Features ## 47: Rename Columns ## 48: Replicate the Input as a Multiplicity ## 49: Center and Scale Numeric Features ## 50: Scale Numeric Features with Respect to their Maximum Absolute Value ## 51: Linearly Transform Numeric Features to Match Given Boundaries ## 52: Remove Features Depending on a Selector ## 53: SMOTE Balancing ## 54: Normalize Data Row-wise ## 55: Subsampling ## 56: Invert Target Transformations ## 57: Transform a Target by a Function ## 58: Linearly Transform a Numeric Target to Match Given Boundaries ## 59: Bag-of-word Representation of Character Features ## 60: Change the Threshold of a Classification Prediction ## 61: Tune the Threshold of a Classification Prediction ## 62: Unbranch Different Paths ## 63: Interface to the vtreat Package ## 64: Yeo-Johnson Transformation of Numeric Features ## label ## packages tags ## 1: mlr3pipelines,bestNormalize data transform ## 2: mlr3pipelines meta ## 3: mlr3pipelines meta ## 4: mlr3pipelines imbalanced data,data transform ## 5: mlr3pipelines,stats ensemble ## 6: mlr3pipelines imbalanced data,data transform ## 7: mlr3pipelines data transform ## 8: mlr3pipelines data transform ## 9: mlr3pipelines data transform ## 10: mlr3pipelines meta ## 11: mlr3pipelines data transform ## 12: mlr3pipelines,stats encode,data transform ## 13: mlr3pipelines encode,data transform ## 14: mlr3pipelines,lme4,nloptr encode,data transform ## 15: mlr3pipelines ensemble ## 16: mlr3pipelines feature selection,data transform ## 17: mlr3pipelines robustify,data transform ## 18: mlr3pipelines,graphics data transform ## 19: mlr3pipelines,fastICA data transform ## 20: mlr3pipelines missings ## 21: mlr3pipelines,graphics missings ## 22: mlr3pipelines missings ## 23: mlr3pipelines missings ## 24: mlr3pipelines,stats missings ## 25: mlr3pipelines missings ## 26: mlr3pipelines missings ## 27: mlr3pipelines missings ## 28: mlr3pipelines,kernlab data transform ## 29: mlr3pipelines learner ## 30: mlr3pipelines learner,ensemble,data transform ## 31: mlr3pipelines missings,data transform ## 32: mlr3pipelines,stats data transform ## 33: mlr3pipelines multiplicity ## 34: mlr3pipelines multiplicity ## 35: mlr3pipelines data transform ## 36: mlr3pipelines,MASS,NMF data transform ## 37: mlr3pipelines meta ## 38: mlr3pipelines target transform,multiplicity ## 39: mlr3pipelines multiplicity,ensemble ## 40: mlr3pipelines data transform ## 41: mlr3pipelines meta ## 42: mlr3pipelines,stats data transform ## 43: mlr3pipelines data transform ## 44: mlr3pipelines abstract ## 45: mlr3pipelines ensemble ## 46: mlr3pipelines robustify,data transform ## 47: mlr3pipelines data transform ## 48: mlr3pipelines multiplicity ## 49: mlr3pipelines data transform ## 50: mlr3pipelines data transform ## 51: mlr3pipelines data transform ## 52: mlr3pipelines feature selection,data transform ## 53: mlr3pipelines,smotefamily imbalanced data,data transform ## 54: mlr3pipelines data transform ## 55: mlr3pipelines data transform ## 56: mlr3pipelines abstract ## 57: mlr3pipelines target transform ## 58: mlr3pipelines target transform ## 59: mlr3pipelines,quanteda,stopwords data transform ## 60: mlr3pipelines target transform ## 61: mlr3pipelines,bbotk target transform ## 62: mlr3pipelines meta ## 63: mlr3pipelines,vtreat encode,missings,data transform ## 64: mlr3pipelines,bestNormalize data transform ## packages tags ## feature_types input.num output.num ## 1: numeric,integer 1 1 ## 2: NA 1 NA ## 3: NA 1 NA ## 4: logical,integer,numeric,character,factor,ordered,... 1 1 ## 5: NA NA 1 ## 6: logical,integer,numeric,character,factor,ordered,... 1 1 ## 7: logical,integer,numeric,character,factor,ordered,... 1 1 ## 8: factor,ordered 1 1 ## 9: logical,integer,numeric,character,factor,ordered,... 1 1 ## 10: NA 1 NA ## 11: POSIXct 1 1 ## 12: factor,ordered 1 1 ## 13: factor,ordered 1 1 ## 14: factor,ordered 1 1 ## 15: NA NA 1 ## 16: logical,integer,numeric,character,factor,ordered,... 1 1 ## 17: factor,ordered 1 1 ## 18: numeric,integer 1 1 ## 19: numeric,integer 1 1 ## 20: logical,integer,numeric,character,factor,ordered,... 1 1 ## 21: integer,numeric 1 1 ## 22: logical,factor,ordered 1 1 ## 23: numeric,integer 1 1 ## 24: numeric,integer 1 1 ## 25: factor,integer,logical,numeric,ordered 1 1 ## 26: character,factor,integer,numeric,ordered 1 1 ## 27: factor,integer,logical,numeric,ordered 1 1 ## 28: numeric,integer 1 1 ## 29: NA 1 1 ## 30: logical,integer,numeric,character,factor,ordered,... 1 1 ## 31: logical,integer,numeric,character,factor,ordered,... 1 1 ## 32: logical,integer,numeric,character,factor,ordered,... 1 1 ## 33: NA 1 NA ## 34: NA NA 1 ## 35: logical,integer,numeric,character,factor,ordered,... 1 1 ## 36: numeric,integer 1 1 ## 37: NA 1 1 ## 38: NA 1 1 ## 39: NA 1 1 ## 40: numeric,integer 1 1 ## 41: NA NA 1 ## 42: numeric,integer 1 1 ## 43: numeric,integer 1 1 ## 44: NA 1 1 ## 45: NA NA 1 ## 46: logical,integer,numeric,character,factor,ordered,... 1 1 ## 47: logical,integer,numeric,character,factor,ordered,... 1 1 ## 48: NA 1 1 ## 49: numeric,integer 1 1 ## 50: numeric,integer 1 1 ## 51: numeric,integer 1 1 ## 52: logical,integer,numeric,character,factor,ordered,... 1 1 ## 53: logical,integer,numeric,character,factor,ordered,... 1 1 ## 54: numeric,integer 1 1 ## 55: logical,integer,numeric,character,factor,ordered,... 1 1 ## 56: NA 2 1 ## 57: NA 1 2 ## 58: NA 1 2 ## 59: character 1 1 ## 60: NA 1 1 ## 61: NA 1 1 ## 62: NA NA 1 ## 63: logical,integer,numeric,character,factor,ordered,... 1 1 ## 64: numeric,integer 1 1 ## feature_types input.num output.num ## input.type.train input.type.predict output.type.train output.type.predict ## 1: Task Task Task Task ## 2: * * * * ## 3: Task Task Task Task ## 4: TaskClassif TaskClassif TaskClassif TaskClassif ## 5: NULL PredictionClassif NULL PredictionClassif ## 6: TaskClassif TaskClassif TaskClassif TaskClassif ## 7: Task Task Task Task ## 8: Task Task Task Task ## 9: Task Task Task Task ## 10: * * * * ## 11: Task Task Task Task ## 12: Task Task Task Task ## 13: Task Task Task Task ## 14: Task Task Task Task ## 15: Task Task Task Task ## 16: Task Task Task Task ## 17: Task Task Task Task ## 18: Task Task Task Task ## 19: Task Task Task Task ## 20: Task Task Task Task ## 21: Task Task Task Task ## 22: Task Task Task Task ## 23: Task Task Task Task ## 24: Task Task Task Task ## 25: Task Task Task Task ## 26: Task Task Task Task ## 27: Task Task Task Task ## 28: Task Task Task Task ## 29: TaskClassif TaskClassif NULL PredictionClassif ## 30: TaskClassif TaskClassif TaskClassif TaskClassif ## 31: Task Task Task Task ## 32: Task Task Task Task ## 33: [*] [*] * * ## 34: * * [*] [*] ## 35: Task Task Task Task ## 36: Task Task Task Task ## 37: * * * * ## 38: TaskClassif TaskClassif [TaskClassif] [TaskClassif] ## 39: [NULL] [PredictionClassif] NULL PredictionClassif ## 40: Task Task Task Task ## 41: * * * * ## 42: Task Task Task Task ## 43: Task Task Task Task ## 44: NULL Prediction NULL Prediction ## 45: NULL PredictionRegr NULL PredictionRegr ## 46: Task Task Task Task ## 47: Task Task Task Task ## 48: * * [*] [*] ## 49: Task Task Task Task ## 50: Task Task Task Task ## 51: Task Task Task Task ## 52: Task Task Task Task ## 53: Task Task Task Task ## 54: Task Task Task Task ## 55: Task Task Task Task ## 56: NULL,NULL function,Prediction NULL Prediction ## 57: Task Task NULL,Task function,Task ## 58: TaskRegr TaskRegr NULL,TaskRegr function,TaskRegr ## 59: Task Task Task Task ## 60: NULL PredictionClassif NULL PredictionClassif ## 61: Task Task NULL Prediction ## 62: * * * * ## 63: Task Task Task Task ## 64: Task Task Task Task ## input.type.train input.type.predict output.type.train output.type.predict 3.2.2 Nodes, Edges and Graphs POs are combined into Graphs. POs are identified by their $id. Note that the operations all modify the object in-place and return the object itself. Therefore, multiple modifications can be chained. Connects PipeOps with edges to control data flow during training and prediction. Input is sent to sources (no in-edges), output is read from sinks (no out-edges). Important methods and slots: Display:print(gr),gr\\$plot(html = TRUE) Accessing PipeOps: gr\\$pipeops Named list of all contained POs. The %&gt;&gt;% operator takes either a PipeOp or a Graph on each of its sides and connects all left-hand outputs to the right-hand inputs. For full control, connect PipeOps explicitly: gr = Graph\\$new() gr\\$add_pipeop(po(&quot;pca&quot;)) gr\\$add_pipeop(lrn(&quot;classif.rpart&quot;)) gr\\$add_edge(&quot;pca&quot;, &quot;classif.rpart&quot;) GraphLearner behave like Learner and enable all mlr3 features: grl = GraphLearner\\$new(gr). See slots $encapsulate for debugging and $model for results after training. Concatenate POs with %&gt;&gt;% to get linear graph. 3.2.3 Modeling The main purpose of a Graph is to build combined preprocessing and model fitting pipelines that can be used as mlr3 Learner. 3.2.3.1 Setting Hyperparameters Individual POs offer hyperparameters because they contain $param_set slots that can be read and written from $param_set$values (via the paradox package). The parameters get passed down to the Graph, and finally to the GraphLearner . This makes it not only possible to easily change the behavior of a Graph / GraphLearner and try different settings manually, but also to perform tuning using the mlr3tuning package. For POs: Exactly as in a Learner. enc = po(&quot;encode&quot;) enc\\$param_set enc\\$param_set\\$values = list(method=&quot;one-hot&quot;) po(&quot;encode&quot;, param_vals = list(method=&quot;one-hot&quot;)) For Graph / GraphLearner: All HPs are collected in a global ParamSet stored in $param_set. IDs are prefixed with the respective PipeOp’s id. 3.2.3.2 Tuning Can jointly tune any Pipeline. Usage of AutoTuner is identical. Details could be seen in below section. 3.2.4 Non-Linear Graphs The Graphs seen so far all have a linear structure. Some POs may have multiple input or output channels. These channels make it possible to create non-linear Graphs with alternative paths taken by the data. Possible types are: Branching: Splitting of a node into several paths, e.g. useful when comparing multiple feature-selection methods (pca, filters). Only one path will be executed. Copying: Splitting of a node into several paths, all paths will be executed (sequentially). Parallel execution is not yet supported. Stacking: Single graphs are stacked onto each other, i.e. the output of one Graph is the input for another. In machine learning this means that the prediction of one Graph is used as input for another Graph. 3.2.4.1 Common functions: gunion() arranges PipeOps or Graphs next to each other in a disjoint graph union. pipeline_greplicate() creates a new Graph containing n copies of the input (PipeOp or Graph). PipeOpFeatureUnion aggregates features from all input tasks into a single Task. 3.2.4.2 Branching &amp; Copying The PipeOpBranch and PipeOpUnbranch POs make it possible to specify multiple alternative paths. Only one path is actually executed, the others are ignored. The active path is determined by a hyperparameter. This concept makes it possible to tune alternative preprocessing paths (or learner models). They controls the path execution. Only one branch can be active. Which one is controlled by a hyperparameter. Unbranching ends the forking. Example: gr = ppl(&quot;branch&quot;, list( pca = po(&quot;pca&quot;), scale = po(&quot;scale&quot;)) ) # set the &quot;pca&quot; path as the active one: gr\\$param_set\\$values\\$branch.selection = &quot;pca&quot; Tuning the branching selection enables powerful model selection. 3.3 Hyperparameter Tuning library(mlr3learners) library(mlr3tuning) The table shows the terminator methods: ## key label properties unit ## 1: clock_time Clock Time single-crit,multi-crit seconds ## 2: combo Combination single-crit,multi-crit percent ## 3: evals Number of Evaluation single-crit,multi-crit evaluations ## 4: none None single-crit,multi-crit percent ## 5: perf_reached Performance Level single-crit percent ## 6: run_time Run Time single-crit,multi-crit seconds ## 7: stagnation Stagnation single-crit percent ## 8: stagnation_batch Stagnation Batch single-crit percent The table shows tuner search strategy we can choose from: as.data.table(mlr_tuners) ## key label ## 1: cmaes Covariance Matrix Adaptation Evolution Strategy ## 2: design_points Design Points ## 3: gensa Generalized Simulated Annealing ## 4: grid_search Grid Search ## 5: irace Iterated Racing ## 6: nloptr Non-linear Optimization ## 7: random_search Random Search ## param_classes ## 1: ParamDbl ## 2: ParamLgl,ParamInt,ParamDbl,ParamFct,ParamUty ## 3: ParamDbl ## 4: ParamLgl,ParamInt,ParamDbl,ParamFct ## 5: ParamDbl,ParamInt,ParamFct,ParamLgl ## 6: ParamDbl ## 7: ParamLgl,ParamInt,ParamDbl,ParamFct ## properties packages ## 1: single-crit mlr3tuning,bbotk,adagio ## 2: dependencies,single-crit,multi-crit mlr3tuning,bbotk ## 3: single-crit mlr3tuning,bbotk,GenSA ## 4: dependencies,single-crit,multi-crit mlr3tuning ## 5: dependencies,single-crit mlr3tuning,bbotk,irace ## 6: single-crit mlr3tuning,bbotk,nloptr ## 7: dependencies,single-crit,multi-crit mlr3tuning,bbotk The parameter set is combined in mutivariate search space SS: ss = ps(&lt;id&gt; = p_int(lower, upper), &lt;id&gt;= p_dbl(lower, upper), &lt;id&gt; = p_dct(levels), &lt;id&gt; = p_lgl()) The &lt;id&gt; represents identifier, and lower, upper, levels are the bounds. Or, we can use to_tune() to set SS for each parameter. To tune by hand, we need to fill define all the arguments in the equation: instance = TuningInstanceSingleCrit$new(task,learner, resampling, measure,terminator, ss) tuner = tnr(&lt;tuner&gt;) We need to use TunningInstanceMultiCrit for multi-criteria tuning. Then we access the results: tuner$optimize(instance) as.data.table(instance$archive) learner$param_set$values = instance$result_learner_param_vals The auto tuner: auto_tuner( method = tnr(&lt;tuner search strategy&gt;), learner = lrn(&lt;learner&gt;, cp = to_tune(lower bound, upperbound, logscale = &lt;TRUE/FALSE&gt;)), resampling = rsmp(&lt;method&gt;), measure = msr(&lt;measure&gt;), term_evals = &lt;#&gt;, batch_size = &lt;#&gt; ) 3.4 Feature Selection library(mlr3fselect) Here is the auto feature selector： The table shows fselectors method we can choose from: as.data.table(mlr_fselectors) ## key label ## 1: design_points Design Points ## 2: exhaustive_search Exhaustive Search ## 3: genetic_search Genetic Search ## 4: random_search Random Search ## 5: rfe Recursive Feature Elimination ## 6: sequential Sequential Search ## 7: shadow_variable_search Shadow Variable Search ## properties packages ## 1: dependencies,single-crit,multi-crit mlr3fselect,bbotk ## 2: single-crit,multi-crit mlr3fselect ## 3: single-crit mlr3fselect ## 4: single-crit,multi-crit mlr3fselect ## 5: single-crit mlr3fselect ## 6: single-crit mlr3fselect ## 7: single-crit mlr3fselect First, we make feature selection by hand, and the process is similar to hyper parameter tuning, we need to define all the arguments and then we can get the result: instance = FSelectInstanceSingleCrit$new(task, learner, resampling, measure, terminator) fselector = fs(&lt;fs method&gt;, batch_size = &lt;number&gt;) fselector$optimize(instance) instance$result We can reselect the features we want using the code: task$select(instance$result_feature_set) Next, we will introduce the auto feature selector that eases the process: autot=auto_fselector( method = &lt;fselector&gt;, learner = &lt;your learner&gt;, resampling = rsmp(&lt;method&gt;), measure = msr(&lt;measure&gt;), term_evals = &lt;#&gt;, batch_size = &lt;#&gt;) autot$train(task, row_ids) autot$predict(task, row_ids) We can check the feature selection subset by calling the learner again: autot$learner "],["sample-for-classification-problems.html", "Chapter 4 Sample for Classification Problems 4.1 Load the R Environment 4.2 Data Description 4.3 Modeling", " Chapter 4 Sample for Classification Problems 4.1 Load the R Environment library(mlr3) library(mlr3learners) library(mlr3viz) library(ggplot2) library(data.table) library(tidyverse) 4.2 Data Description To help readers quickly get started with this package, this section uses the the German credit dataset as an example to show full steps of machine learning. 4.2.1 Load the Data #install.packages(&quot;rchallenge&quot;) data(&quot;german&quot;, package = &quot;rchallenge&quot;) #observe the data glimpse(german) # Data Type ## Rows: 1,000 ## Columns: 21 ## $ status &lt;fct&gt; no checking account, no checking account, ... … ## $ duration &lt;int&gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24, 11, 30, 6… ## $ credit_history &lt;fct&gt; all credits at this bank paid back duly, all c… ## $ purpose &lt;fct&gt; car (used), others, retraining, others, others… ## $ amount &lt;int&gt; 1049, 2799, 841, 2122, 2171, 2241, 3398, 1361,… ## $ savings &lt;fct&gt; unknown/no savings account, unknown/no savings… ## $ employment_duration &lt;fct&gt; &lt; 1 yr, 1 &lt;= ... &lt; 4 yrs, 4 &lt;= ... &lt; 7 yrs, 1 … ## $ installment_rate &lt;ord&gt; &lt; 20, 25 &lt;= ... &lt; 35, 25 &lt;= ... &lt; 35, 20 &lt;= ..… ## $ personal_status_sex &lt;fct&gt; female : non-single or male : single, male : m… ## $ other_debtors &lt;fct&gt; none, none, none, none, none, none, none, none… ## $ present_residence &lt;ord&gt; &gt;= 7 yrs, 1 &lt;= ... &lt; 4 yrs, &gt;= 7 yrs, 1 &lt;= ...… ## $ property &lt;fct&gt; car or other, unknown / no property, unknown /… ## $ age &lt;int&gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, 23, 36, 24… ## $ other_installment_plans &lt;fct&gt; none, none, none, none, bank, none, none, none… ## $ housing &lt;fct&gt; for free, for free, for free, for free, rent, … ## $ number_credits &lt;ord&gt; 1, 2-3, 1, 2-3, 2-3, 2-3, 2-3, 1, 2-3, 1, 2-3,… ## $ job &lt;fct&gt; skilled employee/official, skilled employee/of… ## $ people_liable &lt;fct&gt; 0 to 2, 3 or more, 0 to 2, 3 or more, 0 to 2, … ## $ telephone &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no… ## $ foreign_worker &lt;fct&gt; no, no, no, yes, yes, yes, yes, yes, no, no, n… ## $ credit_risk &lt;fct&gt; good, good, good, good, good, good, good, good… dim(german) # dimension of data ## [1] 1000 21 Through observation, it is found that the dataset has a total of 2000 observations and 21 attributes (columns). The dependent variable we want to predict is creadit_risk (good or bad), and there are 20 independent variables in total, among which duration, age and amount are numerical variables, and the rest are factor variables. skimr packages can be used for a more detailed look at understanding variables. #install.packages(&quot;skimr&quot;) skimr::skim(german) Table 4.1: Data summary Name german Number of rows 1000 Number of columns 21 _______________________ Column type frequency: factor 18 numeric 3 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts status 0 1 FALSE 4 …: 394, no : 274, …: 269, 0&lt;=: 63 credit_history 0 1 FALSE 5 no : 530, all: 293, exi: 88, cri: 49 purpose 0 1 FALSE 10 fur: 280, oth: 234, car: 181, car: 103 savings 0 1 FALSE 5 unk: 603, …: 183, …: 103, 100: 63 employment_duration 0 1 FALSE 5 1 &lt;: 339, &gt;= : 253, 4 &lt;: 174, &lt; 1: 172 installment_rate 0 1 TRUE 4 &lt; 2: 476, 25 : 231, 20 : 157, &gt;= : 136 personal_status_sex 0 1 FALSE 4 mal: 548, fem: 310, fem: 92, mal: 50 other_debtors 0 1 FALSE 3 non: 907, gua: 52, co-: 41 present_residence 0 1 TRUE 4 &gt;= : 413, 1 &lt;: 308, 4 &lt;: 149, &lt; 1: 130 property 0 1 FALSE 4 bui: 332, unk: 282, car: 232, rea: 154 other_installment_plans 0 1 FALSE 3 non: 814, ban: 139, sto: 47 housing 0 1 FALSE 3 ren: 714, for: 179, own: 107 number_credits 0 1 TRUE 4 1: 633, 2-3: 333, 4-5: 28, &gt;= : 6 job 0 1 FALSE 4 ski: 630, uns: 200, man: 148, une: 22 people_liable 0 1 FALSE 2 0 t: 845, 3 o: 155 telephone 0 1 FALSE 2 no: 596, yes: 404 foreign_worker 0 1 FALSE 2 no: 963, yes: 37 credit_risk 0 1 FALSE 2 goo: 700, bad: 300 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist duration 0 1 20.90 12.06 4 12.0 18.0 24.00 72 ▇▇▂▁▁ amount 0 1 3271.25 2822.75 250 1365.5 2319.5 3972.25 18424 ▇▂▁▁▁ age 0 1 35.54 11.35 19 27.0 33.0 42.00 75 ▇▆▃▁▁ 4.3 Modeling When solving the credit risk classification problem by using the mlr3 package, typical problems that arise when building machine learning workflows are: What is the problem we are trying to solve? What is the appropriate learning algorithm? How do we evaluate “good” performance? More systematically in mlr3, they can be represented by five components: Task Definition Learner Definition Training Prediction Performance Evaluation 4.3.1 Task Definition Firstly, we need to determine the goal of the modeling. Most supervised machine learning problems are regression or classification problems. In mlr3, we define tasks to distinguish between these problems. If we want to solve a classification problem, we define a classification task, TaskClassif. For the regression problem, we define a regression task, TaskRegr. In our example, our goal is clearly to model or predict the two-factor variable credit_risk. Therefore, we define a TaskClassif: task = TaskClassif$new(&quot;germancredit&quot;, german , target = &quot;credit_risk&quot;) 4.3.2 Leaner Definition After defining the task, we need to decide how to model. This means we need to decide what learning algorithms or Learners are appropriate. Using prior knowledge (for example, knowing that this is a classification task or assuming that the class is linearly divisible) will eventually result in one or more suitable learners. Many learners are available through the mlr3learners package. In addition, many of the learners are provided via the mlr3extralearners package on GitHub. Together, these two resources account for a large portion of standard learning algorithms. mlr_learners ## &lt;DictionaryLearner&gt; with 27 stored values ## Keys: classif.cv_glmnet, classif.debug, classif.featureless, ## classif.glmnet, classif.kknn, classif.lda, classif.log_reg, ## classif.multinom, classif.naive_bayes, classif.nnet, classif.qda, ## classif.ranger, classif.rpart, classif.svm, classif.xgboost, ## regr.cv_glmnet, regr.debug, regr.featureless, regr.glmnet, regr.kknn, ## regr.km, regr.lm, regr.nnet, regr.ranger, regr.rpart, regr.svm, ## regr.xgboost A suitable learner for our problem could be one of the following: Logistic regression, CART, random forest, etc. The learner can be initialized using the lrn() function and the name of the learner, such as lrn(\" classif.xxx \"). Use mlr_learners_xxx opens the help page for a learner named xxx. For example, logistic regression can be initialized by the following way (logistic regression uses R’s glm() function, provided by the mlr3learners package) : library(&quot;mlr3learners&quot;) learner_logreg = lrn(&quot;classif.log_reg&quot;) print(learner_logreg) ## &lt;LearnerClassifLogReg:classif.log_reg&gt; ## * Model: - ## * Parameters: list() ## * Packages: mlr3, mlr3learners, stats ## * Predict Types: [response], prob ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: loglik, twoclass 4.3.3 Training Training is the process of fitting a model to data. logistic regression Let’s start with an example of logistic regression. However, you will immediately see that this process is very easy to generalize to any learner. You can use $train() to train the initialized learner: learner_logreg$train(task) Typically, in machine learning, we don’t use the full data available, but instead use a subset, the so-called training data. To perform data splitting effectively, you can do the following: train_set = sample(task$row_ids, 0.8 * task$nrow) test_set = setdiff(task$row_ids, train_set) 80% of the data is used for training. The remaining 20% is used for subsequent evaluation. train_set is an integer vector that refers to the selected rows of the original dataset. In mlr3, you can declare training using a subset of the data by attaching the parameter row_ids = train_set: learner_logreg$train(task, row_ids = train_set) The model after training fitting can be displayed through the following commands: learner_logreg$model ## ## Call: stats::glm(formula = task$formula(), family = &quot;binomial&quot;, data = data, ## model = FALSE) ## ## Coefficients: ## (Intercept) ## 0.7393024 ## age ## -0.0103150 ## amount ## 0.0001586 ## credit_historycritical account/other credits elsewhere ## 0.2470500 ## credit_historyno credits taken/all credits paid back duly ## -0.5043088 ## credit_historyexisting credits paid back duly till now ## -0.8117585 ## credit_historyall credits at this bank paid back duly ## -1.2901215 ## duration ## 0.0157534 ## employment_duration&lt; 1 yr ## -0.2407276 ## employment_duration1 &lt;= ... &lt; 4 yrs ## -0.4283570 ## employment_duration4 &lt;= ... &lt; 7 yrs ## -1.0132959 ## employment_duration&gt;= 7 yrs ## -0.3181201 ## foreign_workerno ## 1.4930677 ## housingrent ## -0.8032630 ## housingown ## -0.9057892 ## installment_rate.L ## 0.7182113 ## installment_rate.Q ## 0.0856488 ## installment_rate.C ## -0.0095899 ## jobunskilled - resident ## 0.3889215 ## jobskilled employee/official ## 0.2737369 ## jobmanager/self-empl./highly qualif. employee ## 0.0706391 ## number_credits.L ## -0.2086566 ## number_credits.Q ## -0.3404571 ## number_credits.C ## 0.0561097 ## other_debtorsco-applicant ## 0.4413128 ## other_debtorsguarantor ## -0.6490479 ## other_installment_plansstores ## 0.0882786 ## other_installment_plansnone ## -0.4582146 ## people_liable0 to 2 ## -0.2696636 ## personal_status_sexfemale : non-single or male : single ## -0.2811773 ## personal_status_sexmale : married/widowed ## -0.7107909 ## personal_status_sexfemale : single ## -0.5110095 ## present_residence.L ## 0.1059484 ## present_residence.Q ## -0.4996117 ## present_residence.C ## 0.1126060 ## propertycar or other ## 0.5622138 ## propertybuilding soc. savings agr./life insurance ## 0.2997796 ## propertyreal estate ## 0.8703685 ## purposecar (new) ## -1.5784282 ## purposecar (used) ## -0.6611569 ## purposefurniture/equipment ## -0.5825260 ## purposeradio/television ## -1.1724203 ## purposedomestic appliances ## 0.2088118 ## purposerepairs ## 0.4235540 ## purposevacation ## -1.6464046 ## purposeretraining ## -0.3732084 ## purposebusiness ## -1.6202009 ## savings... &lt; 100 DM ## -0.2514161 ## savings100 &lt;= ... &lt; 500 DM ## -0.7138744 ## savings500 &lt;= ... &lt; 1000 DM ## -1.0130046 ## savings... &gt;= 1000 DM ## -0.9017536 ## status... &lt; 0 DM ## -0.5149461 ## status0&lt;= ... &lt; 200 DM ## -0.8929955 ## status... &gt;= 200 DM / salary for at least 1 year ## -1.7608146 ## telephoneyes (under customer name) ## -0.2743350 ## ## Degrees of Freedom: 799 Total (i.e. Null); 745 Residual ## Null Deviance: 982.4 ## Residual Deviance: 720.9 AIC: 830.9 You can check the type and summary of the model after Logistic regression training: class(learner_logreg$model) ## [1] &quot;glm&quot; &quot;lm&quot; summary(learner_logreg$model) ## ## Call: ## stats::glm(formula = task$formula(), family = &quot;binomial&quot;, data = data, ## model = FALSE) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2577 -0.7179 -0.3859 0.7124 2.5359 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 0.7393024 1.2493778 ## age -0.0103150 0.0101651 ## amount 0.0001586 0.0000495 ## credit_historycritical account/other credits elsewhere 0.2470499 0.6109180 ## credit_historyno credits taken/all credits paid back duly -0.5043088 0.4737427 ## credit_historyexisting credits paid back duly till now -0.8117585 0.5214737 ## credit_historyall credits at this bank paid back duly -1.2901215 0.4843968 ## duration 0.0157534 0.0104132 ## employment_duration&lt; 1 yr -0.2407276 0.4713861 ## employment_duration1 &lt;= ... &lt; 4 yrs -0.4283570 0.4486026 ## employment_duration4 &lt;= ... &lt; 7 yrs -1.0132959 0.4925025 ## employment_duration&gt;= 7 yrs -0.3181201 0.4503263 ## foreign_workerno 1.4930677 0.6354834 ## housingrent -0.8032630 0.2691221 ## housingown -0.9057892 0.5438347 ## installment_rate.L 0.7182114 0.2444771 ## installment_rate.Q 0.0856488 0.2198757 ## installment_rate.C -0.0095899 0.2261949 ## jobunskilled - resident 0.3889215 0.7071465 ## jobskilled employee/official 0.2737369 0.6837783 ## jobmanager/self-empl./highly qualif. employee 0.0706391 0.6909866 ## number_credits.L -0.2086566 0.9494678 ## number_credits.Q -0.3404571 0.7760037 ## number_credits.C 0.0561097 0.5545555 ## other_debtorsco-applicant 0.4413128 0.4517712 ## other_debtorsguarantor -0.6490479 0.4645057 ## other_installment_plansstores 0.0882786 0.4905346 ## other_installment_plansnone -0.4582146 0.2862939 ## people_liable0 to 2 -0.2696636 0.2840207 ## personal_status_sexfemale : non-single or male : single -0.2811773 0.4514250 ## personal_status_sexmale : married/widowed -0.7107909 0.4460512 ## personal_status_sexfemale : single -0.5110095 0.5225887 ## present_residence.L 0.1059484 0.2382842 ## present_residence.Q -0.4996117 0.2265493 ## present_residence.C 0.1126060 0.2199322 ## propertycar or other 0.5622139 0.2899486 ## propertybuilding soc. savings agr./life insurance 0.2997796 0.2691999 ## propertyreal estate 0.8703685 0.4770472 ## purposecar (new) -1.5784282 0.4107929 ## purposecar (used) -0.6611569 0.2989669 ## purposefurniture/equipment -0.5825260 0.2800507 ## purposeradio/television -1.1724203 1.0097400 ## purposedomestic appliances 0.2088118 0.5791930 ## purposerepairs 0.4235540 0.4335391 ## purposevacation -1.6464046 1.2169225 ## purposeretraining -0.3732084 0.3731087 ## purposebusiness -1.6202009 0.8768578 ## savings... &lt; 100 DM -0.2514161 0.3144987 ## savings100 &lt;= ... &lt; 500 DM -0.7138744 0.5200657 ## savings500 &lt;= ... &lt; 1000 DM -1.0130046 0.5648259 ## savings... &gt;= 1000 DM -0.9017536 0.2974495 ## status... &lt; 0 DM -0.5149461 0.2512535 ## status0&lt;= ... &lt; 200 DM -0.8929955 0.4196694 ## status... &gt;= 200 DM / salary for at least 1 year -1.7608146 0.2611354 ## telephoneyes (under customer name) -0.2743350 0.2257623 ## z value Pr(&gt;|z|) ## (Intercept) 0.592 0.554027 ## age -1.015 0.310225 ## amount 3.204 0.001357 ** ## credit_historycritical account/other credits elsewhere 0.404 0.685925 ## credit_historyno credits taken/all credits paid back duly -1.065 0.287093 ## credit_historyexisting credits paid back duly till now -1.557 0.119551 ## credit_historyall credits at this bank paid back duly -2.663 0.007737 ** ## duration 1.513 0.130323 ## employment_duration&lt; 1 yr -0.511 0.609575 ## employment_duration1 &lt;= ... &lt; 4 yrs -0.955 0.339644 ## employment_duration4 &lt;= ... &lt; 7 yrs -2.057 0.039644 * ## employment_duration&gt;= 7 yrs -0.706 0.479926 ## foreign_workerno 2.349 0.018799 * ## housingrent -2.985 0.002838 ** ## housingown -1.666 0.095801 . ## installment_rate.L 2.938 0.003306 ** ## installment_rate.Q 0.390 0.696882 ## installment_rate.C -0.042 0.966182 ## jobunskilled - resident 0.550 0.582328 ## jobskilled employee/official 0.400 0.688914 ## jobmanager/self-empl./highly qualif. employee 0.102 0.918575 ## number_credits.L -0.220 0.826057 ## number_credits.Q -0.439 0.660856 ## number_credits.C 0.101 0.919408 ## other_debtorsco-applicant 0.977 0.328643 ## other_debtorsguarantor -1.397 0.162327 ## other_installment_plansstores 0.180 0.857181 ## other_installment_plansnone -1.601 0.109487 ## people_liable0 to 2 -0.949 0.342392 ## personal_status_sexfemale : non-single or male : single -0.623 0.533373 ## personal_status_sexmale : married/widowed -1.594 0.111044 ## personal_status_sexfemale : single -0.978 0.328152 ## present_residence.L 0.445 0.656587 ## present_residence.Q -2.205 0.027432 * ## present_residence.C 0.512 0.608649 ## propertycar or other 1.939 0.052500 . ## propertybuilding soc. savings agr./life insurance 1.114 0.265453 ## propertyreal estate 1.824 0.068078 . ## purposecar (new) -3.842 0.000122 *** ## purposecar (used) -2.211 0.027003 * ## purposefurniture/equipment -2.080 0.037519 * ## purposeradio/television -1.161 0.245597 ## purposedomestic appliances 0.361 0.718457 ## purposerepairs 0.977 0.328585 ## purposevacation -1.353 0.176080 ## purposeretraining -1.000 0.317181 ## purposebusiness -1.848 0.064641 . ## savings... &lt; 100 DM -0.799 0.424048 ## savings100 &lt;= ... &lt; 500 DM -1.373 0.169857 ## savings500 &lt;= ... &lt; 1000 DM -1.793 0.072896 . ## savings... &gt;= 1000 DM -3.032 0.002432 ** ## status... &lt; 0 DM -2.050 0.040412 * ## status0&lt;= ... &lt; 200 DM -2.128 0.033349 * ## status... &gt;= 200 DM / salary for at least 1 year -6.743 1.55e-11 *** ## telephoneyes (under customer name) -1.215 0.224309 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 982.41 on 799 degrees of freedom ## Residual deviance: 720.91 on 745 degrees of freedom ## AIC: 830.91 ## ## Number of Fisher Scoring iterations: 5 Random Forest Just like logistic regression, we can train a random forest. We use the ranger package to do this quickly. To do this, we first need to define the learner and then actually train it. We now provide additional importance parameters (importance = “permutation”). In doing so, we override the default and let the learner determine the importance of the feature based on the ranking of the importance of the feature: learner_rf = lrn(&quot;classif.ranger&quot;, importance = &quot;permutation&quot;) learner_rf$train(task, row_ids = train_set) learner_rf$importance() ## status duration amount ## 0.0354184416 0.0162090730 0.0123043308 ## credit_history savings property ## 0.0093680090 0.0082123808 0.0077066093 ## employment_duration installment_rate job ## 0.0035315300 0.0027344004 0.0027228244 ## present_residence age other_debtors ## 0.0027105217 0.0026407311 0.0026180095 ## housing purpose number_credits ## 0.0024825109 0.0021642260 0.0015904113 ## people_liable personal_status_sex other_installment_plans ## 0.0010299556 0.0006810170 0.0005749214 ## telephone foreign_worker ## 0.0005376060 0.0001642216 To get a graph of importance values, we convert importance to data.table format and process it with ggplot2: importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE) colnames(importance) = c(&quot;Feature&quot;, &quot;Importance&quot;) ggplot(data=importance, aes(x = reorder(Feature, Importance), y = Importance)) + geom_col() + coord_flip() + xlab(&quot;&quot;) It can be seen that the first seven variables play an important role in predicting the dependent variable. 4.3.4 Prediction Next we will use the trained model to make predictions. After training the model, the model can be used for prediction. In general, prediction is the main purpose of machine learning models. In our case, the model can be used to classify new credit applicants. They are based on the associated credit risk (good and bad) of the feature. Typically, machine learning models predict numerical values. In a regression situation, this is natural. For classification, most models predict scores or probabilities. Based on these values, category predictions can be made. Predict Classes pred_logreg = learner_logreg$predict(task, row_ids = test_set) pred_rf = learner_rf$predict(task, row_ids = test_set) pred_logreg ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response ## 11 good bad ## 21 good good ## 22 good good ## --- ## 995 bad bad ## 996 bad bad ## 997 bad bad pred_rf ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response ## 11 good good ## 21 good good ## 22 good good ## --- ## 995 bad bad ## 996 bad good ## 997 bad bad The $predict() method returns a Prediction object. If you want to use it later, you can convert it to data.table format. We can also display the prediction results in the confusion matrix: pred_logreg$confusion ## truth ## response bad good ## bad 25 16 ## good 32 127 pred_rf$confusion ## truth ## response bad good ## bad 22 9 ## good 35 134 Predict Probabilities Most learning period Learner can not only predict category variables (” response “), but also predict their”confidence”/” uncertainty “degree to a given response. Typically, we do this by setting the Learner’s $predict_type to”prob”. Sometimes this needs to be done before the learner is trained. Alternatively, we can create the learner directly using this option: lrn(\" classif.log_reg \", predict_type= \"prob\") learner_logreg$predict_type = &quot;prob&quot; learner_logreg$predict(task, row_ids = test_set) ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response prob.bad prob.good ## 11 good bad 0.5953696 0.40463043 ## 21 good good 0.4406688 0.55933120 ## 22 good good 0.3221651 0.67783491 ## --- ## 995 bad bad 0.9094086 0.09059142 ## 996 bad bad 0.5018693 0.49813068 ## 997 bad bad 0.5734202 0.42657977 4.3.5 Performance Evaluation To measure the learner’s performance on new data, we usually simulate a sight unseen data by dividing the data into training sets and test sets. The training set is used to train the learner, and the test set is only used to predict and evaluate the performance of the trained learner. Many resampling methods (cross-validation, bootstrap) repeat the segmentation process in different ways. In mlr3, we need to specify the resampling strategy using the rsmp() function: resampling = rsmp(&quot;holdout&quot;, ratio = 2/3) print(resampling) ## &lt;ResamplingHoldout&gt;: Holdout ## * Iterations: 1 ## * Instantiated: FALSE ## * Parameters: ratio=0.6667 In this case, we use a “holdout,” which is a simple train-test split (only one iteration). We use the resample() function for resampling calculation: res = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [03:56:13.614] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/1) res ## &lt;ResampleResult&gt; of 1 iterations ## * Task: germancredit ## * Learner: classif.log_reg ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations The default score for the measure is included in $aggregate(): res$aggregate() ## classif.ce ## 0.2942943 The default metric in this case is classification error. The lower the better. We can run different resampling strategies, such as repeated adherence (” secondary sampling “), or cross-validation. Most methods perform repeated training/prediction cycles on different subsets of data and aggregate the results (usually as averages). Doing this manually requires us to write a loop. mlr3 does the job for us: resampling = rsmp(&quot;subsampling&quot;, repeats=10) rr = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [03:56:13.750] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [03:56:13.793] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [03:56:13.818] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [03:56:13.859] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [03:56:13.896] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [03:56:13.934] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [03:56:13.972] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [03:56:13.998] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [03:56:14.035] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [03:56:14.059] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/10) rr$aggregate() ## classif.ce ## 0.254955 In addition, we can also use cross validation: resampling = resampling = rsmp(&quot;cv&quot;, folds=10) rr = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [03:56:14.120] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [03:56:14.148] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [03:56:14.179] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [03:56:14.206] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [03:56:14.232] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [03:56:14.261] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [03:56:14.287] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [03:56:14.317] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [03:56:14.342] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [03:56:14.391] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/10) rr$aggregate() ## classif.ce ## 0.249 4.3.6 Performance Comparision and Benchmarks We can compare the learner by manually evaluating the resample() for each learning period. However, benchmark() automatically performs resampling evaluations for multiple learners and tasks. benchmark_grid() Create a fully interleaved design: compare multiple learners on multiple tasks. Resampling multiple times. learners = lrns(c(&quot;classif.log_reg&quot;, &quot;classif.ranger&quot;), predict_type = &quot;prob&quot;) bm_design = benchmark_grid( tasks = task, learners = learners, resamplings = rsmp(&quot;cv&quot;, folds = 50) ) bmr = benchmark(bm_design) ## INFO [03:56:14.531] [mlr3] Running benchmark with 100 resampling iterations ## INFO [03:56:14.535] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/50) ## INFO [03:56:14.563] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/50) ## INFO [03:56:14.596] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/50) ## INFO [03:56:14.623] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/50) ## INFO [03:56:14.650] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/50) ## INFO [03:56:14.682] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/50) ## INFO [03:56:14.709] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/50) ## INFO [03:56:14.740] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/50) ## INFO [03:56:14.766] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/50) ## INFO [03:56:14.793] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/50) ## INFO [03:56:14.941] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 11/50) ## INFO [03:56:14.967] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 12/50) ## INFO [03:56:14.994] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 13/50) ## INFO [03:56:15.022] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 14/50) ## INFO [03:56:15.075] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 15/50) ## INFO [03:56:15.104] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 16/50) ## INFO [03:56:15.134] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 17/50) ## INFO [03:56:15.162] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 18/50) ## INFO [03:56:15.191] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 19/50) ## INFO [03:56:15.224] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 20/50) ## INFO [03:56:15.253] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 21/50) ## INFO [03:56:15.281] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 22/50) ## INFO [03:56:15.310] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 23/50) ## INFO [03:56:15.342] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 24/50) ## INFO [03:56:15.369] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 25/50) ## INFO [03:56:15.395] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 26/50) ## INFO [03:56:15.422] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 27/50) ## INFO [03:56:15.448] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 28/50) ## INFO [03:56:15.478] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 29/50) ## INFO [03:56:15.505] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 30/50) ## INFO [03:56:15.531] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 31/50) ## INFO [03:56:15.557] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 32/50) ## INFO [03:56:15.587] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 33/50) ## INFO [03:56:15.613] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 34/50) ## INFO [03:56:15.640] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 35/50) ## INFO [03:56:15.666] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 36/50) ## INFO [03:56:15.696] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 37/50) ## INFO [03:56:15.757] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 38/50) ## INFO [03:56:15.791] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 39/50) ## INFO [03:56:15.820] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 40/50) ## INFO [03:56:15.852] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 41/50) ## INFO [03:56:15.879] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 42/50) ## INFO [03:56:15.906] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 43/50) ## INFO [03:56:15.936] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 44/50) ## INFO [03:56:15.963] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 45/50) ## INFO [03:56:15.990] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 46/50) ## INFO [03:56:16.016] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 47/50) ## INFO [03:56:16.046] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 48/50) ## INFO [03:56:16.073] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 49/50) ## INFO [03:56:16.100] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 50/50) ## INFO [03:56:16.129] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 1/50) ## INFO [03:56:16.340] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 2/50) ## INFO [03:56:16.548] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 3/50) ## INFO [03:56:16.759] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 4/50) ## INFO [03:56:16.970] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 5/50) ## INFO [03:56:17.176] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 6/50) ## INFO [03:56:17.388] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 7/50) ## INFO [03:56:17.597] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 8/50) ## INFO [03:56:17.818] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 9/50) ## INFO [03:56:18.024] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 10/50) ## INFO [03:56:18.231] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 11/50) ## INFO [03:56:18.438] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 12/50) ## INFO [03:56:18.644] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 13/50) ## INFO [03:56:18.849] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 14/50) ## INFO [03:56:19.063] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 15/50) ## INFO [03:56:19.268] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 16/50) ## INFO [03:56:19.473] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 17/50) ## INFO [03:56:19.680] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 18/50) ## INFO [03:56:19.887] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 19/50) ## INFO [03:56:20.101] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 20/50) ## INFO [03:56:20.309] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 21/50) ## INFO [03:56:20.516] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 22/50) ## INFO [03:56:20.721] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 23/50) ## INFO [03:56:20.931] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 24/50) ## INFO [03:56:21.136] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 25/50) ## INFO [03:56:21.343] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 26/50) ## INFO [03:56:21.554] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 27/50) ## INFO [03:56:21.760] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 28/50) ## INFO [03:56:21.977] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 29/50) ## INFO [03:56:22.183] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 30/50) ## INFO [03:56:22.389] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 31/50) ## INFO [03:56:22.593] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 32/50) ## INFO [03:56:22.798] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 33/50) ## INFO [03:56:23.003] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 34/50) ## INFO [03:56:23.214] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 35/50) ## INFO [03:56:23.419] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 36/50) ## INFO [03:56:23.625] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 37/50) ## INFO [03:56:23.829] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 38/50) ## INFO [03:56:24.034] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 39/50) ## INFO [03:56:24.243] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 40/50) ## INFO [03:56:24.448] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 41/50) ## INFO [03:56:24.652] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 42/50) ## INFO [03:56:24.858] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 43/50) ## INFO [03:56:25.069] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 44/50) ## INFO [03:56:25.275] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 45/50) ## INFO [03:56:25.481] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 46/50) ## INFO [03:56:25.700] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 47/50) ## INFO [03:56:25.913] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 48/50) ## INFO [03:56:26.133] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 49/50) ## INFO [03:56:26.339] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 50/50) ## INFO [03:56:26.558] [mlr3] Finished benchmark In benchmarking, we can compare different measures. Here, we look at the misclassification rate and the AUC: measures = msrs(c(&quot;classif.ce&quot;, &quot;classif.auc&quot;)) performances = bmr$aggregate(measures) performances[, c(&quot;learner_id&quot;, &quot;classif.ce&quot;, &quot;classif.auc&quot;)] ## learner_id classif.ce classif.auc ## 1: classif.log_reg 0.248 0.7828609 ## 2: classif.ranger 0.228 0.7974390 4.3.7 Deviating from Hyperparameters Defaults The techniques previously demonstrated build the backbone of the machine learning workflow that features mlr3. However, in most cases, people will never proceed as we did. While many R packages have carefully chosen default Settings, they do not operate optimally under any circumstances. In general, we can choose the value of such a hyperparameter. The learner’s (super) parameter can be accessed and set via its ParamSet $param_set: learner_rf$param_set ## &lt;ParamSet&gt; ## id class lower upper nlevels default ## 1: alpha ParamDbl -Inf Inf Inf 0.5 ## 2: always.split.variables ParamUty NA NA Inf &lt;NoDefault[3]&gt; ## 3: class.weights ParamUty NA NA Inf ## 4: holdout ParamLgl NA NA 2 FALSE ## 5: importance ParamFct NA NA 4 &lt;NoDefault[3]&gt; ## 6: keep.inbag ParamLgl NA NA 2 FALSE ## 7: max.depth ParamInt 0 Inf Inf ## 8: min.node.size ParamInt 1 Inf Inf ## 9: min.prop ParamDbl -Inf Inf Inf 0.1 ## 10: minprop ParamDbl -Inf Inf Inf 0.1 ## 11: mtry ParamInt 1 Inf Inf &lt;NoDefault[3]&gt; ## 12: mtry.ratio ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 13: num.random.splits ParamInt 1 Inf Inf 1 ## 14: num.threads ParamInt 1 Inf Inf 1 ## 15: num.trees ParamInt 1 Inf Inf 500 ## 16: oob.error ParamLgl NA NA 2 TRUE ## 17: regularization.factor ParamUty NA NA Inf 1 ## 18: regularization.usedepth ParamLgl NA NA 2 FALSE ## 19: replace ParamLgl NA NA 2 TRUE ## 20: respect.unordered.factors ParamFct NA NA 3 ignore ## 21: sample.fraction ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 22: save.memory ParamLgl NA NA 2 FALSE ## 23: scale.permutation.importance ParamLgl NA NA 2 FALSE ## 24: se.method ParamFct NA NA 2 infjack ## 25: seed ParamInt -Inf Inf Inf ## 26: split.select.weights ParamUty NA NA Inf ## 27: splitrule ParamFct NA NA 3 gini ## 28: verbose ParamLgl NA NA 2 TRUE ## 29: write.forest ParamLgl NA NA 2 TRUE ## id class lower upper nlevels default ## parents value ## 1: ## 2: ## 3: ## 4: ## 5: permutation ## 6: ## 7: ## 8: ## 9: ## 10: ## 11: ## 12: ## 13: splitrule ## 14: 1 ## 15: ## 16: ## 17: ## 18: ## 19: ## 20: ## 21: ## 22: ## 23: importance ## 24: ## 25: ## 26: ## 27: ## 28: ## 29: ## parents value learner_rf$param_set$values = list(verbose = FALSE) We can choose parameters for our learners in two different ways. If we had a prior knowledge of how the learner should be (hyper-) parameterized, the way to go would be to manually enter the parameters in the parameter set. In most cases, however, we want to tune the learner so that it can search for “good” model configurations on its own. For now, we only want to compare a few models. To see which parameters can be manipulated, we can investigate the original package version’s parameters or look at the learner’s parameter set: as.data.table(learner_rf$param_set)[,.(id, class, lower, upper)] ## id class lower upper ## 1: alpha ParamDbl -Inf Inf ## 2: always.split.variables ParamUty NA NA ## 3: class.weights ParamUty NA NA ## 4: holdout ParamLgl NA NA ## 5: importance ParamFct NA NA ## 6: keep.inbag ParamLgl NA NA ## 7: max.depth ParamInt 0 Inf ## 8: min.node.size ParamInt 1 Inf ## 9: min.prop ParamDbl -Inf Inf ## 10: minprop ParamDbl -Inf Inf ## 11: mtry ParamInt 1 Inf ## 12: mtry.ratio ParamDbl 0 1 ## 13: num.random.splits ParamInt 1 Inf ## 14: num.threads ParamInt 1 Inf ## 15: num.trees ParamInt 1 Inf ## 16: oob.error ParamLgl NA NA ## 17: regularization.factor ParamUty NA NA ## 18: regularization.usedepth ParamLgl NA NA ## 19: replace ParamLgl NA NA ## 20: respect.unordered.factors ParamFct NA NA ## 21: sample.fraction ParamDbl 0 1 ## 22: save.memory ParamLgl NA NA ## 23: scale.permutation.importance ParamLgl NA NA ## 24: se.method ParamFct NA NA ## 25: seed ParamInt -Inf Inf ## 26: split.select.weights ParamUty NA NA ## 27: splitrule ParamFct NA NA ## 28: verbose ParamLgl NA NA ## 29: write.forest ParamLgl NA NA ## id class lower upper For a random forest, two meaningful parameters that control the complexity of the model are num.trees and mtry. num.trees default to 500 and mtry to floor(sqrt(ncol(data) -1)), or 4 in our example. Our goal here is to train three different learners: 1) Default random forest. 2) Random forest with low num.trees and low mtry. 3) Random forest with high num.trees and high mtry. We will benchmark their performance against the German credit data set. To do this, we built three learners and set the parameters accordingly: rf_med = lrn(&quot;classif.ranger&quot;, id = &quot;med&quot;, predict_type = &quot;prob&quot;) rf_low = lrn(&quot;classif.ranger&quot;, id = &quot;low&quot;, predict_type = &quot;prob&quot;, num.trees = 5, mtry = 2) rf_high = lrn(&quot;classif.ranger&quot;, id = &quot;high&quot;, predict_type = &quot;prob&quot;, num.trees = 1000, mtry = 11) Once the learner is defined, we can benchmark them: learners = list(rf_low, rf_med, rf_high) bm_design = benchmark_grid( tasks = task, learners = learners, resamplings = rsmp(&quot;cv&quot;, folds = 10) ) bmr = benchmark(bm_design) ## INFO [03:56:26.834] [mlr3] Running benchmark with 30 resampling iterations ## INFO [03:56:26.837] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [03:56:26.851] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [03:56:26.863] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [03:56:26.875] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [03:56:26.887] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [03:56:26.905] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [03:56:26.918] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [03:56:26.930] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [03:56:26.942] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [03:56:26.953] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [03:56:26.965] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [03:56:27.166] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [03:56:27.361] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [03:56:27.564] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [03:56:27.766] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [03:56:27.967] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [03:56:28.166] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [03:56:28.361] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [03:56:28.555] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [03:56:28.751] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [03:56:28.955] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [03:56:29.540] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [03:56:30.120] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [03:56:30.699] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [03:56:31.275] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [03:56:31.847] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [03:56:32.433] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [03:56:33.007] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [03:56:33.586] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [03:56:34.166] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [03:56:34.763] [mlr3] Finished benchmark bmr ## &lt;BenchmarkResult&gt; of 30 rows with 3 resampling runs ## nr task_id learner_id resampling_id iters warnings errors ## 1 germancredit low cv 10 0 0 ## 2 germancredit med cv 10 0 0 ## 3 germancredit high cv 10 0 0 We can compare the classification error rate and AUC of different learners: measures = msrs(c(&quot;classif.ce&quot;, &quot;classif.auc&quot;)) performances = bmr$aggregate(measures) performances[, .(learner_id, classif.ce, classif.auc)] ## learner_id classif.ce classif.auc ## 1: low 0.288 0.6996411 ## 2: med 0.227 0.7946750 ## 3: high 0.234 0.7940926 autoplot(bmr) Compared with the three parameter tuning models, the default parameter model in this example is better. "],["sample-for-regression-problems.html", "Chapter 5 Sample for Regression Problems 5.1 Visualization", " Chapter 5 Sample for Regression Problems Here we will perform and solve a classification problem using the mlr3 package. First, we load the data and create a machine learning task. library(mlr3) my_data =read.csv(&quot;./winequality-red.csv&quot;, sep = &quot;;&quot;) head(my_data) ## fixed.acidity volatile.acidity citric.acid residual.sugar chlorides ## 1 7.4 0.70 0.00 1.9 0.076 ## 2 7.8 0.88 0.00 2.6 0.098 ## 3 7.8 0.76 0.04 2.3 0.092 ## 4 11.2 0.28 0.56 1.9 0.075 ## 5 7.4 0.70 0.00 1.9 0.076 ## 6 7.4 0.66 0.00 1.8 0.075 ## free.sulfur.dioxide total.sulfur.dioxide density pH sulphates alcohol ## 1 11 34 0.9978 3.51 0.56 9.4 ## 2 25 67 0.9968 3.20 0.68 9.8 ## 3 15 54 0.9970 3.26 0.65 9.8 ## 4 17 60 0.9980 3.16 0.58 9.8 ## 5 11 34 0.9978 3.51 0.56 9.4 ## 6 13 40 0.9978 3.51 0.56 9.4 ## quality ## 1 5 ## 2 5 ## 3 5 ## 4 6 ## 5 5 ## 6 5 We create a machine learning task: my_task = as_task_regr(x = my_data, target = &quot;quality&quot;) my_task ## &lt;TaskRegr:my_data&gt; (1599 x 12) ## * Target: quality ## * Properties: - ## * Features (11): ## - dbl (11): alcohol, chlorides, citric.acid, density, fixed.acidity, ## free.sulfur.dioxide, pH, residual.sugar, sulphates, ## total.sulfur.dioxide, volatile.acidity Next we choose a learner to perform a regression task: learner = mlr_learners$get(&quot;regr.rpart&quot;) learner ## &lt;LearnerRegrRpart:regr.rpart&gt;: Regression Tree ## * Model: - ## * Parameters: xval=0 ## * Packages: mlr3, rpart ## * Predict Types: [response] ## * Feature Types: logical, integer, numeric, factor, ordered ## * Properties: importance, missings, selected_features, weights Since we have 1599 observations, we will split them into test/train using 2:8. train_set = sample(my_task$nrow, 0.8 * my_task$nrow) test_set = setdiff(seq_len(my_task$nrow), train_set) We train the model using the test set: learner$train(my_task, row_ids = train_set) learner$model ## n= 1279 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 1279 863.813900 5.630962 ## 2) alcohol&lt; 11.55 1081 608.105500 5.488437 ## 4) volatile.acidity&gt;=0.385 871 407.628000 5.363949 ## 8) sulphates&lt; 0.555 268 102.667900 5.093284 ## 16) volatile.acidity&gt;=0.8025 46 33.739130 4.695652 * ## 17) volatile.acidity&lt; 0.8025 222 60.148650 5.175676 * ## 9) sulphates&gt;=0.555 603 276.600300 5.484245 ## 18) alcohol&lt; 9.975 294 106.843500 5.292517 * ## 19) alcohol&gt;=9.975 309 148.666700 5.666667 ## 38) total.sulfur.dioxide&lt; 14.5 29 21.862070 5.068966 ## 76) sulphates&lt; 0.625 15 7.733333 4.533333 * ## 77) sulphates&gt;=0.625 14 5.214286 5.642857 * ## 39) total.sulfur.dioxide&gt;=14.5 280 115.371400 5.728571 ## 78) total.sulfur.dioxide&gt;=76 31 7.419355 5.225806 * ## 79) total.sulfur.dioxide&lt; 76 249 99.140560 5.791165 * ## 5) volatile.acidity&lt; 0.385 210 130.995200 6.004762 ## 10) sulphates&lt; 0.645 69 25.072460 5.550725 * ## 11) sulphates&gt;=0.645 141 84.737590 6.226950 ## 22) alcohol&lt; 9.75 29 14.551720 5.655172 * ## 23) alcohol&gt;=9.75 112 58.250000 6.375000 ## 46) pH&gt;=3.255 58 22.568970 6.086207 * ## 47) pH&lt; 3.255 54 25.648150 6.685185 * ## 3) alcohol&gt;=11.55 198 113.863600 6.409091 ## 6) sulphates&lt; 0.615 72 34.875000 5.958333 * ## 7) sulphates&gt;=0.615 126 56.000000 6.666667 * We then predict using the test set: prediction = learner$predict(my_task, row_ids = test_set) prediction$score() ## regr.mse ## 0.4051785 We can select the best feature set by using mlr3fselect package and use the auto tuner: library(mlr3fselect) # auto tuner autos = auto_fselector( method = &quot;random_search&quot;, learner = lrn(&quot;regr.rpart&quot;), resampling = rsmp(&quot;cv&quot;), measure = msr(&quot;regr.mse&quot;), term_evals = 10, batch_size = 5 ) autos$train(my_task, row_ids = train_set) ## INFO [03:56:35.515] [bbotk] Starting to optimize 11 parameter(s) with &#39;&lt;FSelectorRandomSearch&gt;&#39; and &#39;&lt;TerminatorEvals&gt; [n_evals=10, k=0]&#39; ## INFO [03:56:35.516] [bbotk] Evaluating 5 configuration(s) ## INFO [03:56:35.692] [mlr3] Running benchmark with 50 resampling iterations ## INFO [03:56:35.695] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:35.736] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:35.771] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:35.805] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:35.838] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:35.873] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:35.908] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:35.948] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:35.982] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:36.017] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:36.220] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:36.257] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:36.291] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:36.326] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:36.359] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:36.392] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:36.430] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:36.464] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:36.497] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:36.530] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:36.562] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:36.598] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:36.634] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:36.675] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:36.710] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:36.745] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:36.780] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:36.815] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:36.850] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:36.890] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:36.925] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:36.958] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:36.991] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:37.024] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:37.058] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:37.096] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:37.130] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:37.164] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:37.197] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:37.231] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:37.265] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:37.306] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:37.341] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:37.376] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:37.412] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:37.448] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:37.483] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:37.523] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:37.558] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:37.595] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:37.636] [mlr3] Finished benchmark ## INFO [03:56:38.115] [bbotk] Result of batch 1: ## INFO [03:56:38.116] [bbotk] alcohol chlorides citric.acid density fixed.acidity free.sulfur.dioxide pH ## INFO [03:56:38.116] [bbotk] FALSE FALSE FALSE TRUE FALSE TRUE FALSE ## INFO [03:56:38.116] [bbotk] FALSE FALSE FALSE FALSE FALSE TRUE TRUE ## INFO [03:56:38.116] [bbotk] TRUE TRUE TRUE TRUE FALSE TRUE TRUE ## INFO [03:56:38.116] [bbotk] TRUE FALSE FALSE FALSE FALSE FALSE FALSE ## INFO [03:56:38.116] [bbotk] TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## INFO [03:56:38.116] [bbotk] residual.sugar sulphates total.sulfur.dioxide volatile.acidity regr.mse ## INFO [03:56:38.116] [bbotk] TRUE FALSE FALSE FALSE 0.6531754 ## INFO [03:56:38.116] [bbotk] FALSE FALSE FALSE FALSE 0.6781009 ## INFO [03:56:38.116] [bbotk] TRUE TRUE TRUE TRUE 0.4767217 ## INFO [03:56:38.116] [bbotk] FALSE FALSE FALSE TRUE 0.4867314 ## INFO [03:56:38.116] [bbotk] TRUE TRUE FALSE TRUE 0.4701894 ## INFO [03:56:38.116] [bbotk] runtime_learners uhash ## INFO [03:56:38.116] [bbotk] 0.461 db075d73-4253-4eec-8c5c-b8155702704e ## INFO [03:56:38.116] [bbotk] 0.286 eedffa8e-197c-4b14-8256-bca529f96837 ## INFO [03:56:38.116] [bbotk] 0.304 f9e1f3fe-d89f-4a89-91e5-57bcba68cdf6 ## INFO [03:56:38.116] [bbotk] 0.279 b616ad04-e9f7-4101-b8e7-2caef079fcf4 ## INFO [03:56:38.116] [bbotk] 0.306 05f16753-e6ed-4f99-9c11-61c68ad1949b ## INFO [03:56:38.117] [bbotk] Evaluating 5 configuration(s) ## INFO [03:56:38.265] [mlr3] Running benchmark with 50 resampling iterations ## INFO [03:56:38.269] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:38.306] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:38.342] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:38.385] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:38.421] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:38.457] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:38.492] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:38.528] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:38.564] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:38.606] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:38.642] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:38.676] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:38.710] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:38.745] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:38.779] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:38.821] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:38.856] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:38.890] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:38.925] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:38.959] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:38.994] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:39.034] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:39.070] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:39.103] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:39.137] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:39.171] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:39.205] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:39.246] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:39.280] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:39.313] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:39.347] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:39.381] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:39.416] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:39.457] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:39.491] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:39.526] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:39.561] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:39.598] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:39.633] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:39.674] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:39.709] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 1/10) ## INFO [03:56:39.743] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 2/10) ## INFO [03:56:39.778] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 3/10) ## INFO [03:56:39.813] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 4/10) ## INFO [03:56:39.856] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 5/10) ## INFO [03:56:39.894] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 6/10) ## INFO [03:56:39.929] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 7/10) ## INFO [03:56:39.964] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 8/10) ## INFO [03:56:39.998] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 9/10) ## INFO [03:56:40.033] [mlr3] Applying learner &#39;select.regr.rpart&#39; on task &#39;my_data&#39; (iter 10/10) ## INFO [03:56:40.082] [mlr3] Finished benchmark ## INFO [03:56:40.547] [bbotk] Result of batch 2: ## INFO [03:56:40.549] [bbotk] alcohol chlorides citric.acid density fixed.acidity free.sulfur.dioxide pH ## INFO [03:56:40.549] [bbotk] TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## INFO [03:56:40.549] [bbotk] FALSE TRUE TRUE TRUE FALSE TRUE FALSE ## INFO [03:56:40.549] [bbotk] TRUE TRUE FALSE FALSE TRUE FALSE FALSE ## INFO [03:56:40.549] [bbotk] TRUE TRUE TRUE FALSE TRUE TRUE FALSE ## INFO [03:56:40.549] [bbotk] FALSE TRUE TRUE TRUE TRUE TRUE FALSE ## INFO [03:56:40.549] [bbotk] residual.sugar sulphates total.sulfur.dioxide volatile.acidity regr.mse ## INFO [03:56:40.549] [bbotk] TRUE TRUE TRUE TRUE 0.4767217 ## INFO [03:56:40.549] [bbotk] FALSE FALSE FALSE TRUE 0.5665966 ## INFO [03:56:40.549] [bbotk] FALSE FALSE FALSE FALSE 0.5307512 ## INFO [03:56:40.549] [bbotk] TRUE FALSE TRUE FALSE 0.5012905 ## INFO [03:56:40.549] [bbotk] FALSE TRUE FALSE TRUE 0.5563537 ## INFO [03:56:40.549] [bbotk] runtime_learners uhash ## INFO [03:56:40.549] [bbotk] 0.316 cc627ef9-13d7-4533-bbe2-ae47a2ebb917 ## INFO [03:56:40.549] [bbotk] 0.292 62bf355a-3934-4275-aaa7-db20348937b6 ## INFO [03:56:40.549] [bbotk] 0.291 cabd7a96-4446-467a-99f7-9dade5addd51 ## INFO [03:56:40.549] [bbotk] 0.304 710f28bd-591f-4e11-b9fa-a17e26ad5f44 ## INFO [03:56:40.549] [bbotk] 0.299 8a3d540a-5414-4f1a-88de-f0463ffa3781 ## INFO [03:56:40.551] [bbotk] Finished optimizing after 10 evaluation(s) ## INFO [03:56:40.552] [bbotk] Result: ## INFO [03:56:40.552] [bbotk] alcohol chlorides citric.acid density fixed.acidity free.sulfur.dioxide pH ## INFO [03:56:40.552] [bbotk] TRUE TRUE TRUE TRUE TRUE TRUE FALSE ## INFO [03:56:40.552] [bbotk] residual.sugar sulphates total.sulfur.dioxide volatile.acidity ## INFO [03:56:40.552] [bbotk] TRUE TRUE FALSE TRUE ## INFO [03:56:40.552] [bbotk] features ## INFO [03:56:40.552] [bbotk] alcohol,chlorides,citric.acid,density,fixed.acidity,free.sulfur.dioxide,... ## INFO [03:56:40.552] [bbotk] regr.mse ## INFO [03:56:40.552] [bbotk] 0.4701894 Here we can see that the regr.mse increases after performing feature selection, that means it does not get better. 5.1 Visualization We will use mlr3viz to create some visualizations. Autoplot is the default plot for my task. Here it shows a boxplot for the red wine quality. We can see that the interquartile range is between 5-6 and the median is at 5.5. We also view that there are outliers. The resample result prediction plot and the prediction plot can only compare one or two features: ## INFO [03:56:40.719] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 1/5) ## INFO [03:56:40.728] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 2/5) ## INFO [03:56:40.742] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 3/5) ## INFO [03:56:40.753] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 4/5) ## INFO [03:56:40.763] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 5/5) ## INFO [03:56:41.521] [mlr3] Applying learner &#39;regr.rpart&#39; on task &#39;my_data&#39; (iter 1/1) "],["references.html", "Chapter 6 References", " Chapter 6 References Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press. Lang, Michel. 2017. “checkmate: Fast Argument Checks for Defensive R Programming.” The R Journal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028. Funk, et al. (2020, July 27). mlr3gallery: Bike Sharing Demand - Use Case. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-07-27-bikesharing-demand/ Binder &amp; Pfisterer (2020, March 11). mlr3gallery: mlr3tuning Tutorial - German Credit. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/ Pfisterer (2020, April 27). mlr3gallery: A Pipeline for the Titanic Data Set - Advanced. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/ Li, Lisha, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2016. “Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits.” CoRR abs/1603.06560. http://arxiv.org/abs/1603.06560. P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. Schratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. 2019. “Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
