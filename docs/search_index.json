[["index.html", "Community Contribution – mlr3 tutorial Chapter 1 Introduction 1.1 Background 1.2 Target Users 1.3 Development", " Community Contribution – mlr3 tutorial Lingjun Zhang, Michelle Sun 2022-11-15 Chapter 1 Introduction 1.1 Background The mlr3 (Lang et al. 2019) package and ecosystem provide a generic, object-oriented, and extensible framework for classification, regression, survival analysis, and other machine learning tasks for the R language (R Core Team 2019). This unified interface provides functionality to extend and combine existing machine learning algorithms (learners), intelligently select and tune the most appropriate technique for a specific machine learning task, and perform large-scale comparisons that enable meta-learning. Examples of this advanced functionality include hyperparameter tuning and feature selection. Parallelization of many operations is natively supported. 1.2 Target Users We assume that users of mlr3 have the equivalent knowledge of an introductory machine learning course and some experience in R. A background in computer science or statistics will provide a strong basis for understanding the advanced functionality described in the later chapters of this book. “An Introduction to Statistical Learning” provides a comprehensive introduction for those getting started in machine learning. mlr3 is suitable for complex projects that utilize the high degree of control as well as the highly abstracted “syntactic sugar” to mock-up specific tasks. mlr3 provides a domain-specific language for machine learning in R. We target both practitioners who want to quickly apply machine learning algorithms and researchers who want to implement, benchmark, and compare their new methods in a structured environment. 1.3 Development mlr (Bischl et al. 2016) was first released to CRAN in 2013, with the core design and architecture dating back much further. Over time, the addition of many features has led to a considerably more complex design that made it harder to build, maintain, and extend than we had hoped for. With hindsight, we saw that some design and architecture choices in mlr made it difficult to support new features, in particular with respect to pipelines. Furthermore, the R ecosystem as well as helpful packages such as data.table have undergone major changes in the meantime. Bischl, Bernd, Michel Lang, Lars Kotthoff, Julia Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, and Zachary M. Jones. 2016. “mlr: Machine Learning in R.” Journal of Machine Learning Research 17 (170): 1–5. http://jmlr.org/papers/v17/15-066.html. It would have been nearly impossible to integrate all of these changes into the original design of mlr. Instead, we decided to start working on a reimplementation in 2018, which resulted in the first release of mlr3 on CRAN in July 2019. The new design and the integration of further and newly-developed R packages (especially R6, future, and data.table) makes mlr3 much easier to use, maintain, and in many regards more efficient compared to its predecessor mlr. "],["package-prepare.html", "Chapter 2 Package Prepare 2.1 Installation 2.2 Package Ecosystem 2.3 Packages Installation", " Chapter 2 Package Prepare 2.1 Installation We recommend installing the full universe at once: install.packages(&quot;mlr3verse&quot;) You can also just install the base package: install.packages(&quot;mlr3&quot;) 2.2 Package Ecosystem mlr3 makes use of the following packages not developed by core members of the mlr3 team: R6: Reference class objects. data.table: Extension of R’s data.frame. digest: Hash digests. uuid: Unique string identifiers. lgr: Logging facility. mlbench: A collection of machine learning data sets. evaluate: For capturing output, warnings, and exceptions. future / future.apply: For parallelization. These are core packages within the R ecosystem. The mlr3 package itself provides the base functionality that the rest of ecosystem (mlr3verse) rely on and some fundamental building blocks for machine learning. 2.3 Packages Installation All packages in the mlr3 ecosystem can be installed from GitHub and R-universe and the majority (but not all) can be installed from CRAN. We recommend adding the mlr-org R-universe1 to your R options so that you can install all packages with install.packages() without having to worry whether it’s being downloaded from CRAN or R-universe. To do this run the following: usethis::edit_r_profile() And in the file that opens add or change the repos argument in options so it looks something like this (you might need to add the full code block below or just edit the existing options function). options(repos = c( mlrorg = &quot;https://mlr-org.r-universe.dev&quot;, CRAN = &quot;https://cloud.r-project.org/&quot; )) Save the file then restart your R session and you’re ready to go! install.packages(&quot;mlr3verse&quot;) If you want latest development versions of any of our packages you can just run remotes::install_github(&quot;mlr-org/{pkg}&quot;) "],["cheat-sheets.html", "Chapter 3 Cheat Sheets 3.1 Basic Function 3.2 Pipeline 3.3 Hyperparameter Tuning 3.4 Feature Selection", " Chapter 3 Cheat Sheets 3.1 Basic Function 3.2 Pipeline 3.3 Hyperparameter Tuning 3.4 Feature Selection "],["sample-for-classification-problems.html", "Chapter 4 Sample for Classification Problems 4.1 Load the R Environment 4.2 Data Description 4.3 Modeling", " Chapter 4 Sample for Classification Problems 4.1 Load the R Environment library(mlr3) library(mlr3learners) library(mlr3viz) library(ggplot2) library(data.table) library(tidyverse) 4.2 Data Description To help readers quickly get started with this package, this section uses the the German credit dataset as an example to show full steps of machine learning. 4.2.1 Load the Data #install.packages(&quot;rchallenge&quot;) data(&quot;german&quot;, package = &quot;rchallenge&quot;) #observe the data glimpse(german) # Data Type ## Rows: 1,000 ## Columns: 21 ## $ status &lt;fct&gt; no checking account, no checking account, ... … ## $ duration &lt;int&gt; 18, 9, 12, 12, 12, 10, 8, 6, 18, 24, 11, 30, 6… ## $ credit_history &lt;fct&gt; all credits at this bank paid back duly, all c… ## $ purpose &lt;fct&gt; car (used), others, retraining, others, others… ## $ amount &lt;int&gt; 1049, 2799, 841, 2122, 2171, 2241, 3398, 1361,… ## $ savings &lt;fct&gt; unknown/no savings account, unknown/no savings… ## $ employment_duration &lt;fct&gt; &lt; 1 yr, 1 &lt;= ... &lt; 4 yrs, 4 &lt;= ... &lt; 7 yrs, 1 … ## $ installment_rate &lt;ord&gt; &lt; 20, 25 &lt;= ... &lt; 35, 25 &lt;= ... &lt; 35, 20 &lt;= ..… ## $ personal_status_sex &lt;fct&gt; female : non-single or male : single, male : m… ## $ other_debtors &lt;fct&gt; none, none, none, none, none, none, none, none… ## $ present_residence &lt;ord&gt; &gt;= 7 yrs, 1 &lt;= ... &lt; 4 yrs, &gt;= 7 yrs, 1 &lt;= ...… ## $ property &lt;fct&gt; car or other, unknown / no property, unknown /… ## $ age &lt;int&gt; 21, 36, 23, 39, 38, 48, 39, 40, 65, 23, 36, 24… ## $ other_installment_plans &lt;fct&gt; none, none, none, none, bank, none, none, none… ## $ housing &lt;fct&gt; for free, for free, for free, for free, rent, … ## $ number_credits &lt;ord&gt; 1, 2-3, 1, 2-3, 2-3, 2-3, 2-3, 1, 2-3, 1, 2-3,… ## $ job &lt;fct&gt; skilled employee/official, skilled employee/of… ## $ people_liable &lt;fct&gt; 0 to 2, 3 or more, 0 to 2, 3 or more, 0 to 2, … ## $ telephone &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no… ## $ foreign_worker &lt;fct&gt; no, no, no, yes, yes, yes, yes, yes, no, no, n… ## $ credit_risk &lt;fct&gt; good, good, good, good, good, good, good, good… dim(german) # dimension of data ## [1] 1000 21 Through observation, it is found that the dataset has a total of 2000 observations and 21 attributes (columns). The dependent variable we want to predict is creadit_risk (good or bad), and there are 20 independent variables in total, among which duration, age and amount are numerical variables, and the rest are factor variables. skimr packages can be used for a more detailed look at understanding variables. #install.packages(&quot;skimr&quot;) skimr::skim(german) Table 4.1: Data summary Name german Number of rows 1000 Number of columns 21 _______________________ Column type frequency: factor 18 numeric 3 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts status 0 1 FALSE 4 …: 394, no : 274, …: 269, 0&lt;=: 63 credit_history 0 1 FALSE 5 no : 530, all: 293, exi: 88, cri: 49 purpose 0 1 FALSE 10 fur: 280, oth: 234, car: 181, car: 103 savings 0 1 FALSE 5 unk: 603, …: 183, …: 103, 100: 63 employment_duration 0 1 FALSE 5 1 &lt;: 339, &gt;= : 253, 4 &lt;: 174, &lt; 1: 172 installment_rate 0 1 TRUE 4 &lt; 2: 476, 25 : 231, 20 : 157, &gt;= : 136 personal_status_sex 0 1 FALSE 4 mal: 548, fem: 310, fem: 92, mal: 50 other_debtors 0 1 FALSE 3 non: 907, gua: 52, co-: 41 present_residence 0 1 TRUE 4 &gt;= : 413, 1 &lt;: 308, 4 &lt;: 149, &lt; 1: 130 property 0 1 FALSE 4 bui: 332, unk: 282, car: 232, rea: 154 other_installment_plans 0 1 FALSE 3 non: 814, ban: 139, sto: 47 housing 0 1 FALSE 3 ren: 714, for: 179, own: 107 number_credits 0 1 TRUE 4 1: 633, 2-3: 333, 4-5: 28, &gt;= : 6 job 0 1 FALSE 4 ski: 630, uns: 200, man: 148, une: 22 people_liable 0 1 FALSE 2 0 t: 845, 3 o: 155 telephone 0 1 FALSE 2 no: 596, yes: 404 foreign_worker 0 1 FALSE 2 no: 963, yes: 37 credit_risk 0 1 FALSE 2 goo: 700, bad: 300 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist duration 0 1 20.90 12.06 4 12.0 18.0 24.00 72 ▇▇▂▁▁ amount 0 1 3271.25 2822.75 250 1365.5 2319.5 3972.25 18424 ▇▂▁▁▁ age 0 1 35.54 11.35 19 27.0 33.0 42.00 75 ▇▆▃▁▁ 4.3 Modeling When solving the credit risk classification problem by using the mlr3 package, typical problems that arise when building machine learning workflows are: What is the problem we are trying to solve? What is the appropriate learning algorithm? How do we evaluate “good” performance? More systematically in mlr3, they can be represented by five components: Task Definition Learner Definition Training Prediction Performance Evaluation 4.3.1 Task Definition Firstly, we need to determine the goal of the modeling. Most supervised machine learning problems are regression or classification problems. In mlr3, we define tasks to distinguish between these problems. If we want to solve a classification problem, we define a classification task, TaskClassif. For the regression problem, we define a regression task, TaskRegr. In our example, our goal is clearly to model or predict the two-factor variable credit_risk. Therefore, we define a TaskClassif: task = TaskClassif$new(&quot;germancredit&quot;, german , target = &quot;credit_risk&quot;) 4.3.2 Leaner Definition After defining the task, we need to decide how to model. This means we need to decide what learning algorithms or Learners are appropriate. Using prior knowledge (for example, knowing that this is a classification task or assuming that the class is linearly divisible) will eventually result in one or more suitable learners. Many learners are available through the mlr3learners package. In addition, many of the learners are provided via the mlr3extralearners package on GitHub. Together, these two resources account for a large portion of standard learning algorithms. mlr_learners ## &lt;DictionaryLearner&gt; with 27 stored values ## Keys: classif.cv_glmnet, classif.debug, classif.featureless, ## classif.glmnet, classif.kknn, classif.lda, classif.log_reg, ## classif.multinom, classif.naive_bayes, classif.nnet, classif.qda, ## classif.ranger, classif.rpart, classif.svm, classif.xgboost, ## regr.cv_glmnet, regr.debug, regr.featureless, regr.glmnet, regr.kknn, ## regr.km, regr.lm, regr.nnet, regr.ranger, regr.rpart, regr.svm, ## regr.xgboost A suitable learner for our problem could be one of the following: Logistic regression, CART, random forest, etc. The learner can be initialized using the lrn() function and the name of the learner, such as lrn(\" classif.xxx \"). Use mlr_learners_xxx opens the help page for a learner named xxx. For example, logistic regression can be initialized by the following way (logistic regression uses R’s glm() function, provided by the mlr3learners package) : library(&quot;mlr3learners&quot;) learner_logreg = lrn(&quot;classif.log_reg&quot;) print(learner_logreg) ## &lt;LearnerClassifLogReg:classif.log_reg&gt; ## * Model: - ## * Parameters: list() ## * Packages: mlr3, mlr3learners, stats ## * Predict Types: [response], prob ## * Feature Types: logical, integer, numeric, character, factor, ordered ## * Properties: loglik, twoclass 4.3.3 Training Training is the process of fitting a model to data. logistic regression Let’s start with an example of logistic regression. However, you will immediately see that this process is very easy to generalize to any learner. You can use $train() to train the initialized learner: learner_logreg$train(task) Typically, in machine learning, we don’t use the full data available, but instead use a subset, the so-called training data. To perform data splitting effectively, you can do the following: train_set = sample(task$row_ids, 0.8 * task$nrow) test_set = setdiff(task$row_ids, train_set) 80% of the data is used for training. The remaining 20% is used for subsequent evaluation. train_set is an integer vector that refers to the selected rows of the original dataset. In mlr3, you can declare training using a subset of the data by attaching the parameter row_ids = train_set: learner_logreg$train(task, row_ids = train_set) The model after training fitting can be displayed through the following commands: learner_logreg$model ## ## Call: stats::glm(formula = task$formula(), family = &quot;binomial&quot;, data = data, ## model = FALSE) ## ## Coefficients: ## (Intercept) ## 0.7393024 ## age ## -0.0103150 ## amount ## 0.0001586 ## credit_historycritical account/other credits elsewhere ## 0.2470500 ## credit_historyno credits taken/all credits paid back duly ## -0.5043088 ## credit_historyexisting credits paid back duly till now ## -0.8117585 ## credit_historyall credits at this bank paid back duly ## -1.2901215 ## duration ## 0.0157534 ## employment_duration&lt; 1 yr ## -0.2407276 ## employment_duration1 &lt;= ... &lt; 4 yrs ## -0.4283570 ## employment_duration4 &lt;= ... &lt; 7 yrs ## -1.0132959 ## employment_duration&gt;= 7 yrs ## -0.3181201 ## foreign_workerno ## 1.4930677 ## housingrent ## -0.8032630 ## housingown ## -0.9057892 ## installment_rate.L ## 0.7182113 ## installment_rate.Q ## 0.0856488 ## installment_rate.C ## -0.0095899 ## jobunskilled - resident ## 0.3889215 ## jobskilled employee/official ## 0.2737369 ## jobmanager/self-empl./highly qualif. employee ## 0.0706391 ## number_credits.L ## -0.2086566 ## number_credits.Q ## -0.3404571 ## number_credits.C ## 0.0561097 ## other_debtorsco-applicant ## 0.4413128 ## other_debtorsguarantor ## -0.6490479 ## other_installment_plansstores ## 0.0882786 ## other_installment_plansnone ## -0.4582146 ## people_liable0 to 2 ## -0.2696636 ## personal_status_sexfemale : non-single or male : single ## -0.2811773 ## personal_status_sexmale : married/widowed ## -0.7107909 ## personal_status_sexfemale : single ## -0.5110095 ## present_residence.L ## 0.1059484 ## present_residence.Q ## -0.4996117 ## present_residence.C ## 0.1126060 ## propertycar or other ## 0.5622138 ## propertybuilding soc. savings agr./life insurance ## 0.2997796 ## propertyreal estate ## 0.8703685 ## purposecar (new) ## -1.5784282 ## purposecar (used) ## -0.6611569 ## purposefurniture/equipment ## -0.5825260 ## purposeradio/television ## -1.1724203 ## purposedomestic appliances ## 0.2088118 ## purposerepairs ## 0.4235540 ## purposevacation ## -1.6464046 ## purposeretraining ## -0.3732084 ## purposebusiness ## -1.6202009 ## savings... &lt; 100 DM ## -0.2514161 ## savings100 &lt;= ... &lt; 500 DM ## -0.7138744 ## savings500 &lt;= ... &lt; 1000 DM ## -1.0130046 ## savings... &gt;= 1000 DM ## -0.9017536 ## status... &lt; 0 DM ## -0.5149461 ## status0&lt;= ... &lt; 200 DM ## -0.8929955 ## status... &gt;= 200 DM / salary for at least 1 year ## -1.7608146 ## telephoneyes (under customer name) ## -0.2743350 ## ## Degrees of Freedom: 799 Total (i.e. Null); 745 Residual ## Null Deviance: 982.4 ## Residual Deviance: 720.9 AIC: 830.9 You can check the type and summary of the model after Logistic regression training: class(learner_logreg$model) ## [1] &quot;glm&quot; &quot;lm&quot; summary(learner_logreg$model) ## ## Call: ## stats::glm(formula = task$formula(), family = &quot;binomial&quot;, data = data, ## model = FALSE) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2577 -0.7179 -0.3859 0.7124 2.5359 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 0.7393024 1.2493778 ## age -0.0103150 0.0101651 ## amount 0.0001586 0.0000495 ## credit_historycritical account/other credits elsewhere 0.2470499 0.6109180 ## credit_historyno credits taken/all credits paid back duly -0.5043088 0.4737427 ## credit_historyexisting credits paid back duly till now -0.8117585 0.5214737 ## credit_historyall credits at this bank paid back duly -1.2901215 0.4843968 ## duration 0.0157534 0.0104132 ## employment_duration&lt; 1 yr -0.2407276 0.4713861 ## employment_duration1 &lt;= ... &lt; 4 yrs -0.4283570 0.4486026 ## employment_duration4 &lt;= ... &lt; 7 yrs -1.0132959 0.4925025 ## employment_duration&gt;= 7 yrs -0.3181201 0.4503263 ## foreign_workerno 1.4930677 0.6354834 ## housingrent -0.8032630 0.2691221 ## housingown -0.9057892 0.5438347 ## installment_rate.L 0.7182114 0.2444771 ## installment_rate.Q 0.0856488 0.2198757 ## installment_rate.C -0.0095899 0.2261949 ## jobunskilled - resident 0.3889215 0.7071465 ## jobskilled employee/official 0.2737369 0.6837783 ## jobmanager/self-empl./highly qualif. employee 0.0706391 0.6909866 ## number_credits.L -0.2086566 0.9494678 ## number_credits.Q -0.3404571 0.7760037 ## number_credits.C 0.0561097 0.5545555 ## other_debtorsco-applicant 0.4413128 0.4517712 ## other_debtorsguarantor -0.6490479 0.4645057 ## other_installment_plansstores 0.0882786 0.4905346 ## other_installment_plansnone -0.4582146 0.2862939 ## people_liable0 to 2 -0.2696636 0.2840207 ## personal_status_sexfemale : non-single or male : single -0.2811773 0.4514250 ## personal_status_sexmale : married/widowed -0.7107909 0.4460512 ## personal_status_sexfemale : single -0.5110095 0.5225887 ## present_residence.L 0.1059484 0.2382842 ## present_residence.Q -0.4996117 0.2265493 ## present_residence.C 0.1126060 0.2199322 ## propertycar or other 0.5622139 0.2899486 ## propertybuilding soc. savings agr./life insurance 0.2997796 0.2691999 ## propertyreal estate 0.8703685 0.4770472 ## purposecar (new) -1.5784282 0.4107929 ## purposecar (used) -0.6611569 0.2989669 ## purposefurniture/equipment -0.5825260 0.2800507 ## purposeradio/television -1.1724203 1.0097400 ## purposedomestic appliances 0.2088118 0.5791930 ## purposerepairs 0.4235540 0.4335391 ## purposevacation -1.6464046 1.2169225 ## purposeretraining -0.3732084 0.3731087 ## purposebusiness -1.6202009 0.8768578 ## savings... &lt; 100 DM -0.2514161 0.3144987 ## savings100 &lt;= ... &lt; 500 DM -0.7138744 0.5200657 ## savings500 &lt;= ... &lt; 1000 DM -1.0130046 0.5648259 ## savings... &gt;= 1000 DM -0.9017536 0.2974495 ## status... &lt; 0 DM -0.5149461 0.2512535 ## status0&lt;= ... &lt; 200 DM -0.8929955 0.4196694 ## status... &gt;= 200 DM / salary for at least 1 year -1.7608146 0.2611354 ## telephoneyes (under customer name) -0.2743350 0.2257623 ## z value Pr(&gt;|z|) ## (Intercept) 0.592 0.554027 ## age -1.015 0.310225 ## amount 3.204 0.001357 ** ## credit_historycritical account/other credits elsewhere 0.404 0.685925 ## credit_historyno credits taken/all credits paid back duly -1.065 0.287093 ## credit_historyexisting credits paid back duly till now -1.557 0.119551 ## credit_historyall credits at this bank paid back duly -2.663 0.007737 ** ## duration 1.513 0.130323 ## employment_duration&lt; 1 yr -0.511 0.609575 ## employment_duration1 &lt;= ... &lt; 4 yrs -0.955 0.339644 ## employment_duration4 &lt;= ... &lt; 7 yrs -2.057 0.039644 * ## employment_duration&gt;= 7 yrs -0.706 0.479926 ## foreign_workerno 2.349 0.018799 * ## housingrent -2.985 0.002838 ** ## housingown -1.666 0.095801 . ## installment_rate.L 2.938 0.003306 ** ## installment_rate.Q 0.390 0.696882 ## installment_rate.C -0.042 0.966182 ## jobunskilled - resident 0.550 0.582328 ## jobskilled employee/official 0.400 0.688914 ## jobmanager/self-empl./highly qualif. employee 0.102 0.918575 ## number_credits.L -0.220 0.826057 ## number_credits.Q -0.439 0.660856 ## number_credits.C 0.101 0.919408 ## other_debtorsco-applicant 0.977 0.328643 ## other_debtorsguarantor -1.397 0.162327 ## other_installment_plansstores 0.180 0.857181 ## other_installment_plansnone -1.601 0.109487 ## people_liable0 to 2 -0.949 0.342392 ## personal_status_sexfemale : non-single or male : single -0.623 0.533373 ## personal_status_sexmale : married/widowed -1.594 0.111044 ## personal_status_sexfemale : single -0.978 0.328152 ## present_residence.L 0.445 0.656587 ## present_residence.Q -2.205 0.027432 * ## present_residence.C 0.512 0.608649 ## propertycar or other 1.939 0.052500 . ## propertybuilding soc. savings agr./life insurance 1.114 0.265453 ## propertyreal estate 1.824 0.068078 . ## purposecar (new) -3.842 0.000122 *** ## purposecar (used) -2.211 0.027003 * ## purposefurniture/equipment -2.080 0.037519 * ## purposeradio/television -1.161 0.245597 ## purposedomestic appliances 0.361 0.718457 ## purposerepairs 0.977 0.328585 ## purposevacation -1.353 0.176080 ## purposeretraining -1.000 0.317181 ## purposebusiness -1.848 0.064641 . ## savings... &lt; 100 DM -0.799 0.424048 ## savings100 &lt;= ... &lt; 500 DM -1.373 0.169857 ## savings500 &lt;= ... &lt; 1000 DM -1.793 0.072896 . ## savings... &gt;= 1000 DM -3.032 0.002432 ** ## status... &lt; 0 DM -2.050 0.040412 * ## status0&lt;= ... &lt; 200 DM -2.128 0.033349 * ## status... &gt;= 200 DM / salary for at least 1 year -6.743 1.55e-11 *** ## telephoneyes (under customer name) -1.215 0.224309 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 982.41 on 799 degrees of freedom ## Residual deviance: 720.91 on 745 degrees of freedom ## AIC: 830.91 ## ## Number of Fisher Scoring iterations: 5 Random Forest Just like logistic regression, we can train a random forest. We use the ranger package to do this quickly. To do this, we first need to define the learner and then actually train it. We now provide additional importance parameters (importance = “permutation”). In doing so, we override the default and let the learner determine the importance of the feature based on the ranking of the importance of the feature: learner_rf = lrn(&quot;classif.ranger&quot;, importance = &quot;permutation&quot;) learner_rf$train(task, row_ids = train_set) learner_rf$importance() ## status duration amount ## 0.0354184416 0.0162090730 0.0123043308 ## credit_history savings property ## 0.0093680090 0.0082123808 0.0077066093 ## employment_duration installment_rate job ## 0.0035315300 0.0027344004 0.0027228244 ## present_residence age other_debtors ## 0.0027105217 0.0026407311 0.0026180095 ## housing purpose number_credits ## 0.0024825109 0.0021642260 0.0015904113 ## people_liable personal_status_sex other_installment_plans ## 0.0010299556 0.0006810170 0.0005749214 ## telephone foreign_worker ## 0.0005376060 0.0001642216 To get a graph of importance values, we convert importance to data.table format and process it with ggplot2: importance = as.data.table(learner_rf$importance(), keep.rownames = TRUE) colnames(importance) = c(&quot;Feature&quot;, &quot;Importance&quot;) ggplot(data=importance, aes(x = reorder(Feature, Importance), y = Importance)) + geom_col() + coord_flip() + xlab(&quot;&quot;) It can be seen that the first seven variables play an important role in predicting the dependent variable. 4.3.4 Prediction Next we will use the trained model to make predictions. After training the model, the model can be used for prediction. In general, prediction is the main purpose of machine learning models. In our case, the model can be used to classify new credit applicants. They are based on the associated credit risk (good and bad) of the feature. Typically, machine learning models predict numerical values. In a regression situation, this is natural. For classification, most models predict scores or probabilities. Based on these values, category predictions can be made. Predict Classes pred_logreg = learner_logreg$predict(task, row_ids = test_set) pred_rf = learner_rf$predict(task, row_ids = test_set) pred_logreg ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response ## 11 good bad ## 21 good good ## 22 good good ## --- ## 995 bad bad ## 996 bad bad ## 997 bad bad pred_rf ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response ## 11 good good ## 21 good good ## 22 good good ## --- ## 995 bad bad ## 996 bad good ## 997 bad bad The $predict() method returns a Prediction object. If you want to use it later, you can convert it to data.table format. We can also display the prediction results in the confusion matrix: pred_logreg$confusion ## truth ## response bad good ## bad 25 16 ## good 32 127 pred_rf$confusion ## truth ## response bad good ## bad 22 9 ## good 35 134 Predict Probabilities Most learning period Learner can not only predict category variables (” response “), but also predict their”confidence”/” uncertainty “degree to a given response. Typically, we do this by setting the Learner’s $predict_type to”prob”. Sometimes this needs to be done before the learner is trained. Alternatively, we can create the learner directly using this option: lrn(\" classif.log_reg \", predict_type= \"prob\") learner_logreg$predict_type = &quot;prob&quot; learner_logreg$predict(task, row_ids = test_set) ## &lt;PredictionClassif&gt; for 200 observations: ## row_ids truth response prob.bad prob.good ## 11 good bad 0.5953696 0.40463043 ## 21 good good 0.4406688 0.55933120 ## 22 good good 0.3221651 0.67783491 ## --- ## 995 bad bad 0.9094086 0.09059142 ## 996 bad bad 0.5018693 0.49813068 ## 997 bad bad 0.5734202 0.42657977 4.3.5 Performance Evaluation To measure the learner’s performance on new data, we usually simulate a sight unseen data by dividing the data into training sets and test sets. The training set is used to train the learner, and the test set is only used to predict and evaluate the performance of the trained learner. Many resampling methods (cross-validation, bootstrap) repeat the segmentation process in different ways. In mlr3, we need to specify the resampling strategy using the rsmp() function: resampling = rsmp(&quot;holdout&quot;, ratio = 2/3) print(resampling) ## &lt;ResamplingHoldout&gt;: Holdout ## * Iterations: 1 ## * Instantiated: FALSE ## * Parameters: ratio=0.6667 In this case, we use a “holdout,” which is a simple train-test split (only one iteration). We use the resample() function for resampling calculation: res = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [00:55:48.556] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/1) res ## &lt;ResampleResult&gt; of 1 iterations ## * Task: germancredit ## * Learner: classif.log_reg ## * Warnings: 0 in 0 iterations ## * Errors: 0 in 0 iterations The default score for the measure is included in $aggregate(): res$aggregate() ## classif.ce ## 0.2942943 The default metric in this case is classification error. The lower the better. We can run different resampling strategies, such as repeated adherence (” secondary sampling “), or cross-validation. Most methods perform repeated training/prediction cycles on different subsets of data and aggregate the results (usually as averages). Doing this manually requires us to write a loop. mlr3 does the job for us: resampling = rsmp(&quot;subsampling&quot;, repeats=10) rr = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [00:55:48.675] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [00:55:48.724] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [00:55:48.747] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [00:55:48.785] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [00:55:48.825] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [00:55:48.862] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [00:55:48.904] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [00:55:48.927] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [00:55:48.968] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [00:55:48.992] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/10) rr$aggregate() ## classif.ce ## 0.254955 In addition, we can also use cross validation: resampling = resampling = rsmp(&quot;cv&quot;, folds=10) rr = resample(task, learner = learner_logreg, resampling = resampling) ## INFO [00:55:49.050] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [00:55:49.081] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [00:55:49.109] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [00:55:49.139] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [00:55:49.181] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [00:55:49.208] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [00:55:49.238] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [00:55:49.264] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [00:55:49.291] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [00:55:49.341] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/10) rr$aggregate() ## classif.ce ## 0.249 4.3.6 Performance Comparision and Benchmarks We can compare the learner by manually evaluating the resample() for each learning period. However, benchmark() automatically performs resampling evaluations for multiple learners and tasks. benchmark_grid() Create a fully interleaved design: compare multiple learners on multiple tasks. Resampling multiple times. learners = lrns(c(&quot;classif.log_reg&quot;, &quot;classif.ranger&quot;), predict_type = &quot;prob&quot;) bm_design = benchmark_grid( tasks = task, learners = learners, resamplings = rsmp(&quot;cv&quot;, folds = 50) ) bmr = benchmark(bm_design) ## INFO [00:55:49.448] [mlr3] Running benchmark with 100 resampling iterations ## INFO [00:55:49.452] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 1/50) ## INFO [00:55:49.485] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 2/50) ## INFO [00:55:49.529] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 3/50) ## INFO [00:55:49.557] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 4/50) ## INFO [00:55:49.585] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 5/50) ## INFO [00:55:49.612] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 6/50) ## INFO [00:55:49.643] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 7/50) ## INFO [00:55:49.670] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 8/50) ## INFO [00:55:49.696] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 9/50) ## INFO [00:55:49.727] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 10/50) ## INFO [00:55:49.754] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 11/50) ## INFO [00:55:49.780] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 12/50) ## INFO [00:55:49.814] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 13/50) ## INFO [00:55:49.844] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 14/50) ## INFO [00:55:49.897] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 15/50) ## INFO [00:55:49.924] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 16/50) ## INFO [00:55:49.956] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 17/50) ## INFO [00:55:49.983] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 18/50) ## INFO [00:55:50.011] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 19/50) ## INFO [00:55:50.046] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 20/50) ## INFO [00:55:50.075] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 21/50) ## INFO [00:55:50.109] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 22/50) ## INFO [00:55:50.138] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 23/50) ## INFO [00:55:50.171] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 24/50) ## INFO [00:55:50.200] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 25/50) ## INFO [00:55:50.341] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 26/50) ## INFO [00:55:50.367] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 27/50) ## INFO [00:55:50.394] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 28/50) ## INFO [00:55:50.421] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 29/50) ## INFO [00:55:50.448] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 30/50) ## INFO [00:55:50.479] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 31/50) ## INFO [00:55:50.505] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 32/50) ## INFO [00:55:50.532] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 33/50) ## INFO [00:55:50.559] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 34/50) ## INFO [00:55:50.586] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 35/50) ## INFO [00:55:50.616] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 36/50) ## INFO [00:55:50.643] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 37/50) ## INFO [00:55:50.669] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 38/50) ## INFO [00:55:50.695] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 39/50) ## INFO [00:55:50.722] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 40/50) ## INFO [00:55:50.754] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 41/50) ## INFO [00:55:50.781] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 42/50) ## INFO [00:55:50.807] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 43/50) ## INFO [00:55:50.833] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 44/50) ## INFO [00:55:50.859] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 45/50) ## INFO [00:55:50.890] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 46/50) ## INFO [00:55:50.916] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 47/50) ## INFO [00:55:50.942] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 48/50) ## INFO [00:55:50.969] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 49/50) ## INFO [00:55:50.995] [mlr3] Applying learner &#39;classif.log_reg&#39; on task &#39;germancredit&#39; (iter 50/50) ## INFO [00:55:51.025] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 1/50) ## INFO [00:55:51.233] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 2/50) ## INFO [00:55:51.439] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 3/50) ## INFO [00:55:51.648] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 4/50) ## INFO [00:55:51.855] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 5/50) ## INFO [00:55:52.067] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 6/50) ## INFO [00:55:52.274] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 7/50) ## INFO [00:55:52.482] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 8/50) ## INFO [00:55:52.689] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 9/50) ## INFO [00:55:52.896] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 10/50) ## INFO [00:55:53.109] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 11/50) ## INFO [00:55:53.317] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 12/50) ## INFO [00:55:53.524] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 13/50) ## INFO [00:55:53.734] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 14/50) ## INFO [00:55:53.947] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 15/50) ## INFO [00:55:54.155] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 16/50) ## INFO [00:55:54.363] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 17/50) ## INFO [00:55:54.577] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 18/50) ## INFO [00:55:54.784] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 19/50) ## INFO [00:55:54.991] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 20/50) ## INFO [00:55:55.210] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 21/50) ## INFO [00:55:55.417] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 22/50) ## INFO [00:55:55.624] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 23/50) ## INFO [00:55:55.831] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 24/50) ## INFO [00:55:56.039] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 25/50) ## INFO [00:55:56.247] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 26/50) ## INFO [00:55:56.460] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 27/50) ## INFO [00:55:56.667] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 28/50) ## INFO [00:55:56.873] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 29/50) ## INFO [00:55:57.077] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 30/50) ## INFO [00:55:57.285] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 31/50) ## INFO [00:55:57.497] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 32/50) ## INFO [00:55:57.703] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 33/50) ## INFO [00:55:57.910] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 34/50) ## INFO [00:55:58.117] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 35/50) ## INFO [00:55:58.329] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 36/50) ## INFO [00:55:58.537] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 37/50) ## INFO [00:55:58.743] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 38/50) ## INFO [00:55:58.954] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 39/50) ## INFO [00:55:59.160] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 40/50) ## INFO [00:55:59.366] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 41/50) ## INFO [00:55:59.590] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 42/50) ## INFO [00:55:59.798] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 43/50) ## INFO [00:56:00.005] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 44/50) ## INFO [00:56:00.213] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 45/50) ## INFO [00:56:00.420] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 46/50) ## INFO [00:56:00.627] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 47/50) ## INFO [00:56:00.841] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 48/50) ## INFO [00:56:01.047] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 49/50) ## INFO [00:56:01.255] [mlr3] Applying learner &#39;classif.ranger&#39; on task &#39;germancredit&#39; (iter 50/50) ## INFO [00:56:01.474] [mlr3] Finished benchmark In benchmarking, we can compare different measures. Here, we look at the misclassification rate and the AUC: measures = msrs(c(&quot;classif.ce&quot;, &quot;classif.auc&quot;)) performances = bmr$aggregate(measures) performances[, c(&quot;learner_id&quot;, &quot;classif.ce&quot;, &quot;classif.auc&quot;)] ## learner_id classif.ce classif.auc ## 1: classif.log_reg 0.248 0.7828609 ## 2: classif.ranger 0.228 0.7974390 4.3.7 Deviating from Hyperparameters Defaults The techniques previously demonstrated build the backbone of the machine learning workflow that features mlr3. However, in most cases, people will never proceed as we did. While many R packages have carefully chosen default Settings, they do not operate optimally under any circumstances. In general, we can choose the value of such a hyperparameter. The learner’s (super) parameter can be accessed and set via its ParamSet $param_set: learner_rf$param_set ## &lt;ParamSet&gt; ## id class lower upper nlevels default ## 1: alpha ParamDbl -Inf Inf Inf 0.5 ## 2: always.split.variables ParamUty NA NA Inf &lt;NoDefault[3]&gt; ## 3: class.weights ParamUty NA NA Inf ## 4: holdout ParamLgl NA NA 2 FALSE ## 5: importance ParamFct NA NA 4 &lt;NoDefault[3]&gt; ## 6: keep.inbag ParamLgl NA NA 2 FALSE ## 7: max.depth ParamInt 0 Inf Inf ## 8: min.node.size ParamInt 1 Inf Inf ## 9: min.prop ParamDbl -Inf Inf Inf 0.1 ## 10: minprop ParamDbl -Inf Inf Inf 0.1 ## 11: mtry ParamInt 1 Inf Inf &lt;NoDefault[3]&gt; ## 12: mtry.ratio ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 13: num.random.splits ParamInt 1 Inf Inf 1 ## 14: num.threads ParamInt 1 Inf Inf 1 ## 15: num.trees ParamInt 1 Inf Inf 500 ## 16: oob.error ParamLgl NA NA 2 TRUE ## 17: regularization.factor ParamUty NA NA Inf 1 ## 18: regularization.usedepth ParamLgl NA NA 2 FALSE ## 19: replace ParamLgl NA NA 2 TRUE ## 20: respect.unordered.factors ParamFct NA NA 3 ignore ## 21: sample.fraction ParamDbl 0 1 Inf &lt;NoDefault[3]&gt; ## 22: save.memory ParamLgl NA NA 2 FALSE ## 23: scale.permutation.importance ParamLgl NA NA 2 FALSE ## 24: se.method ParamFct NA NA 2 infjack ## 25: seed ParamInt -Inf Inf Inf ## 26: split.select.weights ParamUty NA NA Inf ## 27: splitrule ParamFct NA NA 3 gini ## 28: verbose ParamLgl NA NA 2 TRUE ## 29: write.forest ParamLgl NA NA 2 TRUE ## id class lower upper nlevels default ## parents value ## 1: ## 2: ## 3: ## 4: ## 5: permutation ## 6: ## 7: ## 8: ## 9: ## 10: ## 11: ## 12: ## 13: splitrule ## 14: 1 ## 15: ## 16: ## 17: ## 18: ## 19: ## 20: ## 21: ## 22: ## 23: importance ## 24: ## 25: ## 26: ## 27: ## 28: ## 29: ## parents value learner_rf$param_set$values = list(verbose = FALSE) We can choose parameters for our learners in two different ways. If we had a prior knowledge of how the learner should be (hyper-) parameterized, the way to go would be to manually enter the parameters in the parameter set. In most cases, however, we want to tune the learner so that it can search for “good” model configurations on its own. For now, we only want to compare a few models. To see which parameters can be manipulated, we can investigate the original package version’s parameters or look at the learner’s parameter set: as.data.table(learner_rf$param_set)[,.(id, class, lower, upper)] ## id class lower upper ## 1: alpha ParamDbl -Inf Inf ## 2: always.split.variables ParamUty NA NA ## 3: class.weights ParamUty NA NA ## 4: holdout ParamLgl NA NA ## 5: importance ParamFct NA NA ## 6: keep.inbag ParamLgl NA NA ## 7: max.depth ParamInt 0 Inf ## 8: min.node.size ParamInt 1 Inf ## 9: min.prop ParamDbl -Inf Inf ## 10: minprop ParamDbl -Inf Inf ## 11: mtry ParamInt 1 Inf ## 12: mtry.ratio ParamDbl 0 1 ## 13: num.random.splits ParamInt 1 Inf ## 14: num.threads ParamInt 1 Inf ## 15: num.trees ParamInt 1 Inf ## 16: oob.error ParamLgl NA NA ## 17: regularization.factor ParamUty NA NA ## 18: regularization.usedepth ParamLgl NA NA ## 19: replace ParamLgl NA NA ## 20: respect.unordered.factors ParamFct NA NA ## 21: sample.fraction ParamDbl 0 1 ## 22: save.memory ParamLgl NA NA ## 23: scale.permutation.importance ParamLgl NA NA ## 24: se.method ParamFct NA NA ## 25: seed ParamInt -Inf Inf ## 26: split.select.weights ParamUty NA NA ## 27: splitrule ParamFct NA NA ## 28: verbose ParamLgl NA NA ## 29: write.forest ParamLgl NA NA ## id class lower upper For a random forest, two meaningful parameters that control the complexity of the model are num.trees and mtry. num.trees default to 500 and mtry to floor(sqrt(ncol(data) -1)), or 4 in our example. Our goal here is to train three different learners: 1) Default random forest. 2) Random forest with low num.trees and low mtry. 3) Random forest with high num.trees and high mtry. We will benchmark their performance against the German credit data set. To do this, we built three learners and set the parameters accordingly: rf_med = lrn(&quot;classif.ranger&quot;, id = &quot;med&quot;, predict_type = &quot;prob&quot;) rf_low = lrn(&quot;classif.ranger&quot;, id = &quot;low&quot;, predict_type = &quot;prob&quot;, num.trees = 5, mtry = 2) rf_high = lrn(&quot;classif.ranger&quot;, id = &quot;high&quot;, predict_type = &quot;prob&quot;, num.trees = 1000, mtry = 11) Once the learner is defined, we can benchmark them: learners = list(rf_low, rf_med, rf_high) bm_design = benchmark_grid( tasks = task, learners = learners, resamplings = rsmp(&quot;cv&quot;, folds = 10) ) bmr = benchmark(bm_design) ## INFO [00:56:01.754] [mlr3] Running benchmark with 30 resampling iterations ## INFO [00:56:01.757] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [00:56:01.770] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [00:56:01.787] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [00:56:01.800] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [00:56:01.812] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [00:56:01.824] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [00:56:01.837] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [00:56:01.849] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [00:56:01.861] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [00:56:01.873] [mlr3] Applying learner &#39;low&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [00:56:01.885] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [00:56:02.082] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [00:56:02.280] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [00:56:02.489] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [00:56:02.692] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [00:56:02.887] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [00:56:03.084] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [00:56:03.287] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [00:56:03.483] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [00:56:03.682] [mlr3] Applying learner &#39;med&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [00:56:03.889] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 1/10) ## INFO [00:56:04.488] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 2/10) ## INFO [00:56:05.091] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 3/10) ## INFO [00:56:05.669] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 4/10) ## INFO [00:56:06.249] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 5/10) ## INFO [00:56:06.824] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 6/10) ## INFO [00:56:07.411] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 7/10) ## INFO [00:56:07.989] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 8/10) ## INFO [00:56:08.568] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 9/10) ## INFO [00:56:09.156] [mlr3] Applying learner &#39;high&#39; on task &#39;germancredit&#39; (iter 10/10) ## INFO [00:56:09.753] [mlr3] Finished benchmark bmr ## &lt;BenchmarkResult&gt; of 30 rows with 3 resampling runs ## nr task_id learner_id resampling_id iters warnings errors ## 1 germancredit low cv 10 0 0 ## 2 germancredit med cv 10 0 0 ## 3 germancredit high cv 10 0 0 We can compare the classification error rate and AUC of different learners: measures = msrs(c(&quot;classif.ce&quot;, &quot;classif.auc&quot;)) performances = bmr$aggregate(measures) performances[, .(learner_id, classif.ce, classif.auc)] ## learner_id classif.ce classif.auc ## 1: low 0.288 0.6996411 ## 2: med 0.227 0.7946750 ## 3: high 0.234 0.7940926 autoplot(bmr) Compared with the three parameter tuning models, the default parameter model in this example is better. "],["sample-for-regression-problems.html", "Chapter 5 Sample for Regression Problems", " Chapter 5 Sample for Regression Problems "],["references.html", "Chapter 6 References", " Chapter 6 References Lovelace, Robin, Jakub Nowosad, and Jannes Muenchow. 2019. Geocomputation with r. CRC Press. Lang, Michel. 2017. “checkmate: Fast Argument Checks for Defensive R Programming.” The R Journal 9 (1): 437–45. https://doi.org/10.32614/RJ-2017-028. Funk, et al. (2020, July 27). mlr3gallery: Bike Sharing Demand - Use Case. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-07-27-bikesharing-demand/ Binder &amp; Pfisterer (2020, March 11). mlr3gallery: mlr3tuning Tutorial - German Credit. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-03-11-mlr3tuning-tutorial-german-credit/ Pfisterer (2020, April 27). mlr3gallery: A Pipeline for the Titanic Data Set - Advanced. Retrieved from https://mlr3gallery.mlr-org.com/posts/2020-04-27-mlr3pipelines-Imputation-titanic/ Li, Lisha, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. 2016. “Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits.” CoRR abs/1603.06560. http://arxiv.org/abs/1603.06560. Schratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. 2019. “Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
